---
title: "Practical 3: Text as features: Text classification"
author: "Ayoub Bagheri"
date: "Introduction to Text Mining with R"
mainfont: Arial
fontsize: 12pt
urlcolor: blue
output: 
  html_document:
    code_folding: hide
    toc: true
    toc_depth: 1
    toc_float: true
    df_print: paged
    theme: paper
    pandoc_args: --output=Practical_3.html
---

# Introduction

```{r setup, include=FALSE, warning = FALSE}
knitr::opts_chunk$set(collapse = TRUE)
```

Welcome to the third practical of the course "Introduction to Text Mining with R". In this practical we will start with creating a pipeline for text classification using simple training and test sets. In the second part of the practical, we will train machine learning techniques on a news dataset and we will compare them.

In this practical we are going to use the following packages:

```{r load_packages, message = FALSE, warning = FALSE}
library(caret)
library(tidyverse)
library(tm)
library(e1071)
```

---

# Document-term matrix

---

---

## A simple pipeline

--- 

1. **In this part, we will use two sentences to train a model and one sentence to test it. Assume our data consists of two sentences:**
- 'Cats like to chase mice.', 
- 'Dogs like to eat big bones.'

|     **Use these sentences as our training set and create a document-term matrix (dtm). Also remove punctuations, stopwords, numbers and apply stemming. Hint: Use the tm package. To create a dtm first you need to create a vector space model.**

---

```{r 1, include = TRUE}
toy_data_train <- c('Cats like to chase mice.', 
                    'Dogs like to eat big bones.')

# convert data to vector space model
toy_corpus_train <- VCorpus(VectorSource(toy_data_train))

# create a dtm object
toy_dtm_train <- DocumentTermMatrix(toy_corpus_train, 
                                    list(removePunctuation = TRUE, 
                                         stopwords = TRUE, 
                                         stemming = TRUE, 
                                         removeNumbers = TRUE))
```

---

2. **Inspect the toy_dtm_train object.**

---


```{r 2, include = TRUE}
inspect(toy_dtm_train)
#inspect(toy_dtm_train[1:rownumber, 1:colnumber])

```

---

3. **Convert the toy_dtm_train object to a matrix and add a 'y' column as the outcome variable which is 0 for the first row and 1 for the second row. Then, convert the toy_dtm_train object to a dataframe.**

---

```{r 3, include = TRUE}
toy_data_train <- cbind(as.matrix(toy_dtm_train), c(0, 1))

colnames(toy_data_train)[ncol(toy_data_train)] <- 'y'
toy_data_train <- as.data.frame(toy_data_train)
toy_data_train$y <- as.factor(toy_data_train$y)
toy_data_train

```


---

4. **From the 'caret' package, use the train function with method = 'bayesglm' and fit a model on the toy_dtm_train object. For 'bayesglm', you may need to install Package ‘arm’ which is a library for data analysis using regression and multilevel/hierarchical models.**

---

```{r 4, include = TRUE, warning = FALSE}
fit_toy <- train(y ~ ., data = toy_data_train, method = 'bayesglm')
summary(fit_toy)
```

---

5. **Use the fitted model to predict the outcome on the training data.**

---

```{r 5, include = TRUE, warning = FALSE}
predict(fit_toy)

```

Here, you see that the model perfectly predicts the category for our training data.

---

6. **Use the sentence "Rats eat cheese." as your test data. What is the prediction of your model? What steps do you need to get the prediction?**

---

```{r 6, include = TRUE}
toy_test_sentence <- c('Rats eat cheese.')

# convert to vector space model
toy_corpus_test <- VCorpus(VectorSource(toy_test_sentence))

# convert to a dtm
toy_dtm_test <- DocumentTermMatrix(toy_corpus_test, 
                                   control = list(dictionary = Terms(toy_dtm_train), # remember that we need to use the terminology from the training data         
                                                  removePunctuation = TRUE, 
                                                  stopwords = TRUE, 
                                                  stemming = TRUE, 
                                                  removeNumbers = TRUE))
toy_test_data <- as.matrix(toy_dtm_test)

# predict category for the test data
predict(fit_toy, newdata = toy_test_data)

```

---

7. **Now we want to use a tf-idf weighting to create our dtm object. Remember to remove punctuations, stopwords, numbers and apply the stemming. What is different in this new tf-idf-based dtm? and in your prediction? Repeat the analysis in Q.3 to Q.6 with the new object.**

---

```{r 7, include = TRUE, warning = FALSE}
# create a dtm object
tou_dtm_train_tfidf <- DocumentTermMatrix(toy_corpus_train, 
                                          list(weighting = weightTfIdf,
                                               removePunctuation = TRUE, 
                                               stopwords = TRUE, 
                                               stemming = TRUE, 
                                               removeNumbers = TRUE))

# create the y variable
toy_train_data_tfidf <-cbind(as.matrix(tou_dtm_train_tfidf), c(0, 1))
colnames(toy_train_data_tfidf)[ncol(toy_train_data_tfidf)] <- 'y'
toy_train_data_tfidf <- as.data.frame(toy_train_data_tfidf)
toy_train_data_tfidf$y <- as.factor(toy_train_data_tfidf$y)
toy_train_data_tfidf

# fit the naive Bayes model with the tfidf representation
toy_fit_tfidf <- train(y ~ ., data = toy_train_data_tfidf, method = 'bayesglm')
summary(toy_fit_tfidf)

# prediction on training data
predict(toy_fit_tfidf)

# prediction on test data
# convert to a dtm
tou_dtm_test_tfidf <- DocumentTermMatrix(toy_corpus_test, 
                                        control = list(dictionary = Terms(tou_dtm_train_tfidf), # remember that we need to use the vocabulary from the training data
                                                       weighting = weightTfIdf,
                                                       removePunctuation = TRUE, 
                                                       stopwords = TRUE, 
                                                       stemming = TRUE, 
                                                       removeNumbers = TRUE))
toy_test_data_tfidf <- as.matrix(tou_dtm_test_tfidf)

# predict category for the test data
predict(toy_fit_tfidf, newdata = toy_test_data_tfidf)


```

---

# Compare architectures: Naïve Bayes versus SVM

The data set used in this part of the practical is the BBC News data set. The raw data set can be downloaded from [here](http://mlg.ucd.ie/datasets/bbc.html).
It consists of 2225 documents from the BBC news website corresponding to stories in five topical areas from 2004 to 2005. These areas are:

- Business
- Entertainment
- Politics
- Sport
- Tech

---

8. **Load the data set from the data folder and inspect its first rows.**

Note that the loaded data set object is called `df_final`.

---
```{r 8, include = TRUE, warning = FALSE}
load("data/news_dataset.rda")
head(df_final)

#can take a subset (e.g. 20% of the original data set) to reduce computational time
set.seed(123)
df_final <- sample_frac(df_final, 0.2)
```

---

9. **Find out about the name of the categories and the number of observations in each of them.**

---

```{r 9, include = TRUE, warning = FALSE}
# list of the categories in the data set
unique(df_final$Category)
table(df_final$Category)

```


---

10. **Create a document-term matrix for the entire dataset and save the terms with the frequency higher than 10 in a new variable and name it features. In this process, convert the word into lowercase, remove punctuations, numbers, stopwords, whitespaces and clean any other data issues.**

---

```{r 10, include = TRUE, warning = FALSE}
corpus_BBC <- Corpus(VectorSource(df_final$Content))

# standardize to lowercase
corpus_BBC <- tm_map(corpus_BBC, content_transformer(tolower)) 
# returns error invalid multibyte string 1512
# Two possible solutions: 
# 1) Omission of the whole observation
# df_final <- df_final[-1512,]
# 2) Omission of the string "\xa315.8m" from that specifc text
# df_final[1512,][2] <- gsub("\xa315.8m", "", df_final[1512,][2])
# then rerun from line 255.

# remove tm stopwords
corpus_BBC <- tm_map(corpus_BBC, removeWords, stopwords())
# standardize whitespaces
corpus_BBC <- tm_map(corpus_BBC, stripWhitespace)
# remove punctuation
corpus_BBC <- tm_map(corpus_BBC, removePunctuation)
# remove numbers
corpus_BBC <- tm_map(corpus_BBC, removeNumbers)

dtm_BBC <- DocumentTermMatrix(corpus_BBC)

# words appearing more than 10x
features_BBC <- findFreqTerms(dtm_BBC, 10)

head(features_BBC)
```

---

11. **Partition the original data and the corpus object into training and test sets with 80% for the training set and 20% for the test set.**

---

```{r 11, include = TRUE, warning = FALSE}
## set the seed to make your partition reproducible
set.seed(123)

train_idx <- createDataPartition(df_final$Category, p=0.80, list=FALSE)
# set for the original raw data 
train_BBC_raw <- df_final[train_idx,]
test_BBC_raw <- df_final[-train_idx,]
# set for the cleaned-up data
train_BBC_clean <- corpus_BBC[train_idx]
test_BBC_clean <- corpus_BBC[-train_idx]

```

---

12. **Create separate document-term matrices for the training and the test sets using the features variable as dictionary and convert them into data frames.**

---

```{r 12, include = TRUE, warning = FALSE}
dtm_BBC_train <- DocumentTermMatrix(train_BBC_clean, list(dictionary=features_BBC))
dtm_BBC_test  <- DocumentTermMatrix(test_BBC_clean, list(dictionary=features_BBC))

dtm_BBC_train <- as.data.frame(as.matrix(dtm_BBC_train))
dtm_BBC_test <- as.data.frame(as.matrix(dtm_BBC_test))
str(dtm_BBC_test)

```

---

<!-- 13. **Use the code below to find out how frequently terms appear by summing the content of all terms, and then plot the tdidf frequencies.** -->

<!-- --- -->

<!-- ```{r 13, include = TRUE, warning = FALSE} -->
<!-- freq <- colSums(dtm_BBC_train) -->

<!-- head(freq, 10) -->
<!-- tail(freq,10) -->

<!-- ``` -->

<!-- ```{r} -->
<!-- plot(sort(freq, decreasing = T), -->
<!--      col  = "blue", -->
<!--      main = "Word TF frequencies",  -->
<!--      xlab = "TF-based rank",  -->
<!--      ylab = "TF") -->

<!-- ``` -->

---

13. **Show 20 most frequent terms and their frequencies in a bar plot.**

---

```{r 14, include = TRUE}
freq         <- colSums(dtm_BBC_train)
high_freq    <- tail(sort(freq), n = 20)
hfp_df       <- as.data.frame(sort(high_freq))
hfp_df$names <- rownames(hfp_df) 

ggplot(hfp_df, aes(reorder(names, high_freq), high_freq)) +
  geom_bar(stat="identity") + 
  coord_flip() + 
  xlab("Terms") + 
  ylab("Frequency") +
  ggtitle("Term frequencies")
```

---

14. **Use the cbind function to add the categories to the dtm_train data and name the column y.**

---

```{r 15, include = TRUE}
dtm_BBC_train <- cbind(cat=factor(train_BBC_raw$Category), dtm_BBC_train)
dtm_BBC_test <- cbind(cat=factor(test_BBC_raw$Category), dtm_BBC_test)
dtm_BBC_train<-as.data.frame(dtm_BBC_train)
dtm_BBC_test<-as.data.frame(dtm_BBC_test)

```

---

15. **Fit a naive Bayes model on the training data set and name it fit_BBC_nb. Check the summary of the fitted model and predict the categories for the training data.**

---


```{r 16, include = TRUE, warning = FALSE}
# gc()
# Naive Bayes with Laplace smoothing
fit_BBC_nb <- naiveBayes(cat~., data=dtm_BBC_train, laplace = 1)
summary(fit_BBC_nb)

pred_BBC_nb_train <- predict(fit_BBC_nb, na.omit(dtm_BBC_train))
fit_BBC_nb_table <- table(na.omit(dtm_BBC_train$cat), pred_BBC_nb_train, dnn=c("Actual", "Predicted"))
fit_BBC_nb_table
```

The predict function allows you to specify whether you want the most probable class or if you want to get the probability for every class. Nothing changes with the exception being the type parameter is set to “raw”.

---

16. **Fit a SVM model with a linear kernel on the training data set and name it fit_BBC_svm. Check the summary of the fitted model and predict the categories for the training data.**

---

```{r 17, include = TRUE, warning = FALSE}
fit_BBC_svm <- svm(cat~., data=dtm_BBC_train)
summary(fit_BBC_svm)

pred_BBC_svm_train <- predict(fit_BBC_svm, na.omit(dtm_BBC_train))
fit_BBC_svm_table <- table(na.omit(dtm_BBC_train$cat), pred_BBC_svm_train, dnn=c("Actual", "Predicted"))
fit_BBC_svm_table

```


---

17. **Now prepare the test data for the models, then check the prediction performances.**

---

```{r 18, include = TRUE}
# prediction on test data
pred_BBC_nb_test <- predict(fit_BBC_nb, na.omit(dtm_BBC_test))
fit_BBC_nb_test <- table(na.omit(dtm_BBC_test$cat), pred_BBC_nb_test, dnn=c("Actual", "Predicted"))
fit_BBC_nb_test
```


```{r svm_test, include = TRUE, warning = FALSE}
# prediction on test data
pred_BBC_svm_test <- predict(fit_BBC_svm, na.omit(dtm_BBC_test))
fit_BBC_svm_test <- table(na.omit(dtm_BBC_test$cat), pred_BBC_svm_test, dnn=c("Actual", "Predicted"))
fit_BBC_svm_test
```

---

18. **Calculate Accuracy, Sensitivity, Specificity, Pos Pred Value, and Neg Pred Value for the test set using each of the two models.**

---

The baseline performance (accuracy) is: 
```{r baseline, include = TRUE}
table(dtm_BBC_train$cat)
cat("\n The baseline prediction accuracy: ")
mean(na.omit(dtm_BBC_test$cat) == "sport")
```

```{r 19, include = TRUE}
confusionMatrix(fit_BBC_nb_test)

```

```{r 20, include = TRUE}
confusionMatrix(fit_BBC_svm_test)
```

---

# Summary

---

In this practical, we learned about:

- Documet-term matrix representation
- tf and tfidf methods
- Naive Bayes and SVM
- Model comparison

---

End of Practical

---
