---
title: "Practical 3: Text as features: Text classification"
author: "Ayoub Bagheri"
date: "Introduction to Text Mining with R"
mainfont: Arial
fontsize: 12pt
urlcolor: blue
output: 
  html_document:
    code_folding: hide
    toc: true
    toc_depth: 1
    toc_float: true
    df_print: paged
    theme: paper
    pandoc_args: --output=Practical_3.html
---

# Introduction

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE)
```

Welcome to the third practical of the course "Introduction to Text Mining with R". In this practical we will start with creating a pipeline for text classification using simple training and test sets. In the second part of the practical, we will train machine learning techniques on a news dataset and we will compare them with using a feature selection method.

In this practical we are going to use the following packages:

```{r load_packages, message = FALSE, warning = FALSE}
library(caret)
library(tidyverse)
library(tm)
library(dplyr)
library(e1071)
# install.packages("SnowballC")

```

---

# Document-term matrix

---

---

## A simple pipeline

--- 

1. **In this part, we will use two sentences to train a model and one sentence to test it. Assume our data consists of two sentences:**
- 'Cats like to chase mice.', 
- 'Dogs like to eat big bones.'

|     **Use these sentences as our training set and create a document-term matrix (dtm). Also remove punctuations, stopwords, numbers and apply stemming. Hint: Use the tm package. To create a dtm first you need to create a vector space model.**

---

```{r 1, include = TRUE}
data <- c('Cats like to chase mice.', 
          'Dogs like to eat big bones.')

# convert data to vector space model
corpus <- VCorpus(VectorSource(data))

# create a dtm object
dtm <- DocumentTermMatrix(corpus, 
                          list(removePunctuation = TRUE, 
                               stopwords = TRUE, 
                               stemming = TRUE, 
                               removeNumbers = TRUE))

```

---

2. **Inspect the dtm object.**

---


```{r 2, include = TRUE}
inspect(dtm)
#inspect(dtm[1:rownumber, 1:colnumber])

```

---

3. **Convert the dtm object to a matrix and add a 'y' column as the outcome variable which is 0 for the first row and 1 for the second row. Then, convert the dtm object to a dataframe.**

---

```{r 3, include = TRUE}
train <- as.matrix(dtm)
train <- cbind(train, c(0, 1))

colnames(train)[ncol(train)] <- 'y'
train <- as.data.frame(train)
train$y <- as.factor(train$y)
train

```


---

4. **From the 'caret' package, use the train function with method = 'bayesglm' and fit a model on the dtm object. For 'bayesglm', you may need to install Package ‘arm’ which is a library for data analysis using regression and multilevel/hierarchical models.**

---

```{r 4, include = TRUE, warning = FALSE}
fit <- train(y ~ ., data = train, method = 'bayesglm')
summary(fit)

```

---

5. **Use the fitted model to predict the outcome on the training data.**

---

```{r 5, include = TRUE, warning = FALSE}
predict(fit)

```

Here, you see that the model perfectly predicts the category for our training data.

---

6. **Use the sentence "Rats eat cheese." as your test data. What is the prediction of your model? What steps do you need to get the prediction?**

---

```{r 6, include = TRUE}
test_sentence <- c('Rats eat cheese.')

# convert to vector space model
corpus <- VCorpus(VectorSource(test_sentence))

# convert to a dtm
dtm <- DocumentTermMatrix(corpus, 
                          control = list(dictionary = Terms(dtm), # remember that we need to use the terminology from the training data
                                         removePunctuation = TRUE, 
                                         stopwords = TRUE, 
                                         stemming = TRUE, 
                                         removeNumbers = TRUE))
test_data <- as.matrix(dtm)

# predict category for the test data
predict(fit, newdata = test_data)

```

---

7. **Now we want to use a tf-idf weighting to create our dtm object. Remember to remove punctuations, stopwords, numbers and apply the stemming. What is different in this new tf-idf-based dtm? and in your prediction? Repeat the analysis in Q.3 to Q.6 with the new object.**

---

```{r 7, include = TRUE, warning = FALSE}
# convert data to vector space model
corpus <- VCorpus(VectorSource(data))

# create a dtm object
dtm_tfidf <- DocumentTermMatrix(corpus, 
                                list(weighting = weightTfIdf,
                                     removePunctuation = TRUE, 
                                     stopwords = TRUE, 
                                     stemming = TRUE, 
                                     removeNumbers = TRUE))

# create the y variable
train <- as.matrix(dtm_tfidf)
train <- cbind(train, c(0, 1))
colnames(train)[ncol(train)] <- 'y'
train <- as.data.frame(train)
train$y <- as.factor(train$y)
train

# fit the naive Bayes model with the tfidf representation
fit_by_tfidf <- train(y ~ ., data = train, method = 'bayesglm')
summary(fit_by_tfidf)

# prediction on training data
predict(fit_by_tfidf)

# prediction on test data
# convert to vector space model
corpus <- VCorpus(VectorSource(test_sentence))
# convert to a dtm
dtm <- DocumentTermMatrix(corpus, 
                          control = list(dictionary = Terms(dtm_tfidf), # remember that we need to use the vocabulary from the training data
                                         weighting = weightTfIdf,
                                         removePunctuation = TRUE, 
                                         stopwords = TRUE, 
                                         stemming = TRUE, 
                                         removeNumbers = TRUE))
test_data <- as.matrix(dtm)

# predict category for the test data
predict(fit, newdata = test_data)


```

---

# Compare architectures: Naïve Bayes versus SVM

The data set used in this part of the practical is the BBC News data set. The raw data set can be downloaded from [here](http://mlg.ucd.ie/datasets/bbc.html).
It consists of 2225 documents from the BBC news website corresponding to stories in five topical areas from 2004 to 2005. These areas are:

- Business
- Entertainment
- Politics
- Sport
- Tech

---

8. **Load the data set from the data folder and inspect its first rows.**

---
```{r 8, include = TRUE, warning = FALSE}
load("data/news_dataset.rda")
head(df_final)

```

---

9. **Find out about the name of the categories and the number of observations in each of them.**

---

```{r 9, include = TRUE, warning = FALSE}
# list of the categories in the data set
unique(df_final$Category)
table(df_final$Category)

```


---

10. **Create a document-term matrix for the entire dataset and save the terms with the frequency higher than 10 in a new variable and name it features. In this process, convert the word into lowercase, remove punctuations, numbers, stopwords, whitespaces and clean any other data issues.**

---

```{r 10, include = TRUE, warning = FALSE}
## set the seed to make your partition reproducible
set.seed(123)

corpus <- Corpus(VectorSource(df_final$Content))

# standardize to lowercase
corpus <- tm_map(corpus, content_transformer(tolower)) 
# returns error invalid multibyte string 1512
# Two possible solutions: 
# 1) Omission of the whole observation
# df_final <- df_final[-1512,]
# 2) Omission of the string "\xa315.8m" from that specifc text
# df_final[1512,][2] <- gsub("\xa315.8m", "", df_final[1512,][2])
# then rerun from line 255.

# remove tm stopwords
corpus <- tm_map(corpus, removeWords, stopwords())
# standardize whitespaces
corpus <- tm_map(corpus, stripWhitespace)
# remove punctuation
corpus <- tm_map(corpus, removePunctuation)
# remove numbers
corpus <- tm_map(corpus, removeNumbers)

dtm <- DocumentTermMatrix(corpus)

# words appearing more than 10x
features <- findFreqTerms(dtm, 10)

head(features)

```

---

11. **Partition the original data and the corpus object into training and test sets with 80% for the training set and 20% for the test set.**

---

```{r 11, include = TRUE, warning = FALSE}

train_idx <- createDataPartition(df_final$Category, p=0.80, list=FALSE)
# set for the original raw data 
train1 <- df_final[train_idx,]
test1 <- df_final[-train_idx,]
# set for the cleaned-up data
train2 <- corpus[train_idx]
test2 <- corpus[-train_idx]

```
---


12. **Create separate document-term matrices for the training and the test sets using the features variable as dictionary and convert them into data frames.**

---

```{r 12, include = TRUE, warning = FALSE}
dtm_train <- DocumentTermMatrix(train2, list(dictionary=features))
dtm_test  <- DocumentTermMatrix(test2, list(dictionary=features))

dtm_train <- as.data.frame(as.matrix(dtm_train))
dtm_test <- as.data.frame(as.matrix(dtm_test))
str(dtm_test)


```

---

13. **Use the code below to find out how frequently terms appear by summing the content of all terms, and then plot the tdidf frequencies.**

---

```{r 13, include = TRUE, warning = FALSE}

freq <- colSums(dtm_train)

head(freq, 10)
tail(freq,10)

```

```{r}
plot(sort(freq, decreasing = T),
     col  = "blue",
     main = "Word TF frequencies", 
     xlab = "TF-based rank", 
     ylab = "TF")

```

---

14. **Show 20 most frequent terms and their frequencies in a bar plot.**

---

```{r 14, include = TRUE}
high_freq    <- tail(sort(freq), n = 20)
hfp_df       <- as.data.frame(sort(high_freq))
hfp_df$names <- rownames(hfp_df) 

ggplot(hfp_df, aes(reorder(names, high_freq), high_freq)) +
  geom_bar(stat="identity") + 
  coord_flip() + 
  xlab("Terms") + 
  ylab("Frequency") +
  ggtitle("Term frequencies")

```

---

15. **Use the cbind function to add the categories to the dtm_train data and name the column y.**

---

```{r 15, include = TRUE}
dtm_train1 <- cbind(cat=factor(train1$Category), dtm_train)
dtm_test1 <- cbind(cat=factor(test1$Category), dtm_test)
dtm_train1<-as.data.frame(dtm_train1)
dtm_test1<-as.data.frame(dtm_test1)

```

---

16. **Fit a naive Bayes model on the training data set and name it fit_nb. Check the summary of the fitted model and predict the categories for the training data.**

---


```{r 16, include = TRUE}
# gc()
# Naive Bayes with Laplace smoothing
fit_nb <- naiveBayes(cat~., data=dtm_train1, laplace = 1)
summary(fit_nb)

pred_nb_train <- predict(fit_nb, na.omit(dtm_train1))
fit_nb_table <- table(na.omit(dtm_train1$cat), pred_nb_train, dnn=c("Actual", "Predicted"))
fit_nb_table


```

The predict function allows you to specify whether you want the most probable class or if you want to get the probability for every class. Nothing changes with the exception being the type parameter is set to “raw”.

---

17. **Fit a SVM model with a linear kernel on the training data set and name it fit_svm. Check the summary of the fitted model and predict the categories for the training data.**

---

```{r 17, include = TRUE}
fit_svm <- svm(cat~., data=dtm_train1)
summary(fit_svm)

pred_svm_train <- predict(fit_svm, na.omit(dtm_train1))
fit_svm_table <- table(na.omit(dtm_train1$cat), pred_svm_train, dnn=c("Actual", "Predicted"))
fit_svm_table

```



---

18. **Now prepare the test data for the models, then check the prediction performances.**

---

```{r 18, include = TRUE}
# prediction on test data
pred_nb_test <- predict(fit_nb, na.omit(dtm_test1))
fit_nb_test <- table(na.omit(dtm_test1$cat), pred_nb_test, dnn=c("Actual", "Predicted"))
fit_nb_test

```


```{r svm_test, include = TRUE}
# prediction on test data
pred_svm_test <- predict(fit_svm, na.omit(dtm_test1))
fit_svm_test <- table(na.omit(dtm_test1$cat), pred_svm_test, dnn=c("Actual", "Predicted"))
fit_svm_test

```

---

19. **Calculate Accuracy, Sensitivity, Specificity, Pos Pred Value, and Neg Pred Value for the test set using each of the two models.**

---

```{r 19, include = TRUE}
confusionMatrix(fit_svm_test)

```

---

# Summary

---

In this practical, we learned about:

- Documet-term matrix representation
- tf and tfidf methods
- Naive Bayes and SVM
- Model comparison

---

End of Practical

---
