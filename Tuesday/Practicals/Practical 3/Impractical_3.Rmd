---
title: "Practical 3: Text as features: Text classification"
author: "Ayoub Bagheri"
date: "Introduction to Text Mining with R"
mainfont: Arial
fontsize: 12pt
urlcolor: blue
output: 
  html_document:
    code_folding: hide
    toc: true
    toc_depth: 1
    toc_float: true
    df_print: paged
    theme: paper
    pandoc_args: --output=Practical_3.html
---

# Introduction

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE)
```

Welcome to the third practical of the course "Introduction to Text Mining with R". In this practical we will start with creating a pipeline for text classification using simple training and test sets. In the second part of the practical, we will train machine learning techniques on a news dataset and we will compare them with using a feature selection method.

In this practical we are going to use the following packages:

```{r load_packages, message = FALSE, warning = FALSE}
library(caret)
library(tidyverse)
library(tm)
library(dplyr)
library(e1071)
# install.packages("SnowballC")

```

---

# Document-term matrix

---

---

## A simple pipeline

--- 

1. **In this part, we will use two sentences to train a model and one sentence to test it. Assume our data consists of two sentences:**
- 'Cats like to chase mice.', 
- 'Dogs like to eat big bones.'

|     **Use these sentences as our training set and create a document-term matrix (dtm). Also remove punctuations, stopwords, numbers and apply stemming. Hint: Use the tm package. To create a dtm first you need to create a vector space model.**

---

```{r 1, include = TRUE}


```

---

2. **Inspect the dtm object.**

---


```{r 2, include = TRUE}


```

---

3. **Convert the dtm object to a matrix and add a 'y' column as the outcome variable which is 0 for the first row and 1 for the second row. Then, convert the dtm object to a dataframe.**

---

```{r 3, include = TRUE}


```


---

4. **From the 'caret' package, use the train function with method = 'bayesglm' and fit a model on the dtm object. For 'bayesglm', you may need to install Package ‘arm’ which is a library for data analysis using regression and multilevel/hierarchical models.**

---

```{r 4, include = TRUE, warning = FALSE}


```

---

5. **Use the fitted model to predict the outcome on the training data.**

---

```{r 5, include = TRUE, warning = FALSE}


```

Here, you see that the model perfectly predicts the category for our training data.

---

6. **Use the sentence "Rats eat cheese." as your test data. What is the prediction of your model? What steps do you need to get the prediction?**

---

```{r 6, include = TRUE}


```

---

7. **Now we want to use a tf-idf weighting to create our dtm object. Remember to remove punctuations, stopwords, numbers and apply the stemming. What is different in this new tf-idf-based dtm? and in your prediction? Repeat the analysis in Q.3 to Q.6 with the new object.**

---

```{r 7, include = TRUE, warning = FALSE}


```

---

# Compare architectures: Naïve Bayes versus SVM

The data set used in this part of the practical is the BBC News data set. The raw data set can be downloaded from [here](http://mlg.ucd.ie/datasets/bbc.html).
It consists of 2225 documents from the BBC news website corresponding to stories in five topical areas from 2004 to 2005. These areas are:

- Business
- Entertainment
- Politics
- Sport
- Tech

---

8. **Load the data set from the data folder and inspect its first rows.**

---
```{r 8, include = TRUE, warning = FALSE}

```

---

9. **Find out about the name of the categories and the number of observations in each of them.**

---

```{r 9, include = TRUE, warning = FALSE}

```


---

10. **Create a document-term matrix for the entire dataset and save the terms with the frequency higher than 10 in a new variable and name it features. In this process, convert the word into lowercase, remove punctuations, numbers, stopwords, whitespaces and clean any other data issues.**

---

```{r 10, include = TRUE, warning = FALSE}


```

---

11. **Partition the original data and the corpus object into training and test sets with 80% for the training set and 20% for the test set.**

---

```{r 11, include = TRUE, warning = FALSE}


```
---


12. **Create separate document-term matrices for the training and the test sets using the features variable as dictionary and convert them into data frames.**

---

```{r 12, include = TRUE, warning = FALSE}


```

---

13. **Use the code below to find out how frequently terms appear by summing the content of all terms, and then plot the tdidf frequencies.**

---

```{r 13, include = TRUE, warning = FALSE}


```

```{r}

```

---

14. **Show 20 most frequent terms and their frequencies in a bar plot.**

---

```{r 14, include = TRUE}


```

---

15. **Use the cbind function to add the categories to the dtm_train data and name the column y.**

---

```{r 15, include = TRUE}


```

---

16. **Fit a naive Bayes model on the training data set and name it fit_nb. Check the summary of the fitted model and predict the categories for the training data.**

---


```{r 16, include = TRUE}


```

The predict function allows you to specify whether you want the most probable class or if you want to get the probability for every class. Nothing changes with the exception being the type parameter is set to “raw”.

---

17. **Fit a SVM model with a linear kernel on the training data set and name it fit_svm. Check the summary of the fitted model and predict the categories for the training data.**

---

```{r 17, include = TRUE}


```



---

18. **Now prepare the test data for the models, then check the prediction performances.**

---

```{r 18, include = TRUE}


```


```{r svm_test, include = TRUE}


```

---

19. **Calculate Accuracy, Sensitivity, Specificity, Pos Pred Value, and Neg Pred Value for the test set using each of the two models.**

---

```{r 19, include = TRUE}

```

---

# Summary

---

In this practical, we learned about:

- Documet-term matrix representation
- tf and tfidf methods
- Naive Bayes and SVM
- Model comparison

---

End of Practical

---
