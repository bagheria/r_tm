---
title: "Sentiment analysis"
mainfont: Arial
fontsize: 12pt
urlcolor: blue
output: 
  html_document:
    code_folding: hide
    toc: true
    toc_depth: 1
    toc_float: true
    df_print: paged
    theme: paper
    pandoc_args: --output=Practical_4.html
---

# Introduction

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE)
```

Welcome to the fourth practical of the course "Introduction to Text Mining with R". In this practical, we will analyze sentiments in a text data set, first by using dictionaries and then we will classify sentiment in text reviews comparing a dictionary-based method and a neural network model.

In this practical we are going to use the following packages:

```{r load_packages, message = FALSE, warning = FALSE}
library(tm)
library(tidytext)
library(dplyr)
library(ggplot2)
library(caret)
library(rpart)
library(rpart.plot)
```

---

# Read data

---

1. **In today's practical, we want to use Taylor Swift song lyrics data from all her albums. Read the "taylor_swift.csv" dataset from the data folder.**

---

```{r 1, message = FALSE, warning = FALSE, include = TRUE}
taylor_swift_lyrics <- read.csv("data/taylor_swift_lyrics.csv")

```

---

2. **First we must preprocess the corpus. Create a document-term matrix from the `lyrics` column of the `ts` data frame. Complete the following preprocessing steps:**
- convert to lower
- remove stop words
- remove numbers
- remove punctuation.

---


```{r 2, message = FALSE, warning = FALSE, include = TRUE}
docs <- Corpus(VectorSource(taylor_swift_lyrics$Lyrics))

dtm <- DocumentTermMatrix(docs,
           control = list(tolower = TRUE,
                          removeNumbers = TRUE,
                          removePunctuation = TRUE,
                          stopwords = TRUE
                         ))

```

---

3. **Inspect the dtm object and convert it to a dataframe.**

---

```{r 3, message = FALSE, warning = FALSE, include = TRUE}
# inspect(dtm)
dtm <- as.data.frame(as.matrix(dtm))

```


---

# Sentiment dictionaries

---

4. **We're going to use sentiment dictionaries from the `tidytext` package. Using the `get_sentiments` function, load the "bing" and "afinn" dictionaries and store them in two objects called `bing_sentiments` and `afinn_sentiments`. You might need to install the "textdata" package.**

The tidytext package contains 4 general purpose lexicons in the sentiments dataset.

AFINN - listing of english words rated for valence between -5 and +5

bing - listing of positive and negative sentiment

nrc - list of English words and their associations with 8 emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and 2 sentiments (negative and positive); binary

loughran - list of sentiment words for accounting and finance by category (Negative, Positive, Uncertainty, Litigious, Strong Modal, Weak Modal, Constraining)

---

```{r 4, message = FALSE, warning = FALSE, include = TRUE}
# also “nrc”, “loughran”

bing_sentiments  <- get_sentiments("bing")
afinn_sentiments <- get_sentiments("afinn")

head(bing_sentiments)
head(afinn_sentiments)

```

---

5. **The afinn_sentiments has the rating for valence between -5 and +5, while the bing_sentiments contains listing of positive and negative sentiment. Add a column to `bing_sentiments` called `score`. This column should hold a "1" for positive words and "-1" for negative words.**

---

```{r 5, message = FALSE, warning = FALSE, include = TRUE}
bing_sentiments$score <- ifelse(bing_sentiments$sentiment=="positive", 1, -1)

```

---

# Sentiment score for each lyric

---

6. **Create a dataframe that holds all the words in the dtm object along with their sentiment score.**

---

```{r 6, message = FALSE, warning = FALSE, include = TRUE}
# get all the words in our dtm and put it in a dataframe
words <- data.frame(word = colnames(dtm))
head(words)

# get their sentiment scores
words <- merge(words, bing_sentiments, all.x = T)
head(words)

# replace NAs with 0s
words$score[is.na(words$score)] <- 0
head(words)

```

---

7. **To calculate a score for each lyric, multiply your dtm object by the scoring dataframe.**

---

```{r 7, message = FALSE, warning = FALSE, include = TRUE}

# calculate documents scores with matrix algebra! 
scores <- as.matrix(dtm) %*% words$score

```

---

8. **Add the calculated scores for each lyric to the original `taylor_swift_lyrics` dataframe.**

---

```{r 8, message = FALSE, warning = FALSE, include = TRUE}
taylor_swift_lyrics$sentiment_bing <- scores
head(taylor_swift_lyrics)

```

---

9. **Plot the bing sentiment scores for each lyric.**

---

```{r 9, message = FALSE, warning = FALSE, include = TRUE}
taylor_swift_lyrics[1:60,] %>% ggplot() +
  geom_col(aes(Title, sentiment_bing), fill = "lightgreen", alpha=.75) +
  theme(legend.position = "none", 
        plot.title = element_text(hjust = 0.5),
        panel.grid.major = element_blank()) +
  xlab("") + 
  ylab("Sentiment Score") +
  ggtitle("Sentiment Analysis of Taylor Swift Lyrics") +
  coord_flip() +
  theme_minimal()

taylor_swift_lyrics[61:132,] %>% ggplot() +
  geom_col(aes(Title, sentiment_bing), fill = "lightgreen", alpha=.75) +
  theme(legend.position = "none", 
        plot.title = element_text(hjust = 0.5),
        panel.grid.major = element_blank()) +
  xlab("") + 
  ylab("Sentiment Score") +
  ggtitle("Sentiment Analysis of Taylor Swift Lyrics") +
  coord_flip() +
  theme_minimal()

```

---

10. **Using the code you wrote above, below we made a function that gets 1) a vector of texts, and 2) a sentiment dictionary (i.e. a dataframe with words and scores), and returns a vector of sentiment scores for each text. Use this function to repeat your analysis with the `afinn` sentiment dictionary.**

---

```{r 10, message = FALSE, warning = FALSE, include = TRUE}
sentiment_score <- function(texts, sentiment_dict){
  # preprocess texts
  docs <- Corpus(VectorSource(texts))
  dtm <- DocumentTermMatrix(docs,
           control = list(stopwords = T,
                          tolower = TRUE,
                          removeNumbers = TRUE,
                          removePunctuation = TRUE))
  dtm <- as.data.frame(as.matrix(dtm))
  
  # get all the words in dtm and put it in a dataframe
  words <- data.frame(word = colnames(dtm))

  # get their sentiment scores
  words <- merge(words, sentiment_dict, all.x = T)

  # replace NAs with 0s
  # words$score[is.na(words$score)] <- 0
  words$score[is.na(words$score)] <- 0
  
  # calculate documents scores with matrix algebra!
  scores <- as.matrix(dtm) %*% words$score
  
  return(scores)
}
```

```{r 10+1, message = FALSE, warning = FALSE, include = TRUE}
colnames(afinn_sentiments)[2] <- "score"
taylor_swift_lyrics$sentiment_afinn <- sentiment_score(taylor_swift_lyrics$Lyrics, afinn_sentiments)

```

```{r 10+2, message = FALSE, warning = FALSE, include = TRUE}
taylor_swift_lyrics[1:60,] %>% ggplot() +
  geom_col(aes(Title, sentiment_afinn), fill = "orange", alpha=.75) +
  theme(legend.position = "none", 
        plot.title = element_text(hjust = 0.5),
        panel.grid.major = element_blank()) +
  xlab("") + 
  ylab("Sentiment Score") +
  ggtitle("Sentiment Analysis of Taylor Swift Lyrics") +
  coord_flip() +
  theme_minimal()

taylor_swift_lyrics[61:132,] %>% ggplot() +
  geom_col(aes(Title, sentiment_afinn), fill = "orange", alpha=.75) +
  theme(legend.position = "none", 
        plot.title = element_text(hjust = 0.5),
        panel.grid.major = element_blank()) +
  xlab("") + 
  ylab("Sentiment Score") +
  ggtitle("Sentiment Analysis of Taylor Swift Lyrics") +
  coord_flip() +
  theme_minimal()
```

---

11. **Compare the bing and afinn dictionaries by finding out which the most and least positive Taylor Swift album is. You can also plot the sentiments for albums.**

---

```{r 11, message = FALSE, warning = FALSE, include = TRUE}
# concatenate to make albums
albums <- taylor_swift_lyrics %>% group_by(Album) %>%
  summarise(lyrics = paste0(Lyrics, collapse = ";"))

# add sentiments
albums$sentiment_bing <- sentiment_score(albums$lyrics, bing_sentiments)

# concatenate to make albums
# albums <- taylor_swift_lyrics %>% group_by(Album) %>%
#   summarise(lyrics = paste0(Lyrics, collapse = ";"))

# add sentiments
albums$sentiment_afinn <- sentiment_score(albums$lyrics, afinn_sentiments)

albums %>% ggplot() +
  geom_col(aes(Album, sentiment_bing), fill = "#edc948", alpha=.75) +
  theme(legend.position = "none", 
        plot.title = element_text(hjust = 0.5),
        panel.grid.major = element_blank()) +
  xlab("") + 
  ylab("Sentiment Score") +
  ggtitle("Sentiment Analysis of Taylor Swift Albums using the Bing dictionary") +
  coord_flip() +
  theme_minimal()

albums %>% ggplot() +
  geom_col(aes(Album, sentiment_afinn), fill = "lightblue", alpha=.75) +
  theme(legend.position = "none", 
        plot.title = element_text(hjust = 0.5),
        panel.grid.major = element_blank()) +
  xlab("") + 
  ylab("Sentiment Score") +
  ggtitle("Sentiment Analysis of Taylor Swift Albums using the Afinn dictionary") +
  coord_flip() +
  theme_minimal()

```

---

# Sentiment analysis of product reviews
In this part of the practical, we will do some sentiment analysis on computer product reviews. For this purpose, we will use our processed data set from the first practical: the computer_531 dataset.

---

12. **Load the final `computer_531` dataframe from the first practical and  to read data from the 'computer.txt' file.**

---

```{r 12, message = FALSE, warning = FALSE, include = TRUE}

computer_531 <- read.csv("computer_531.csv")

```

---

13. **Apply the `sentiment_score` function on the reviews in the dataframe with both bing and afinn dictionaries.**

---

```{r 13, message = FALSE, warning = FALSE, include = TRUE}
computer_531$sentiment_bing  <- sentiment_score(computer_531$review, bing_sentiments)
computer_531$sentiment_afinn <- sentiment_score(computer_531$review, afinn_sentiments)

head(computer_531[,c("review", "sentiment", "sentiment_bing", "sentiment_afinn")], n = 10)

```

---

14. **Create a confusion matrix, and calculate Accuracy, precision, recall and F1 measures for the output of the bing dictionary.**

---

```{r 14, message = FALSE, warning = FALSE, include = TRUE}
# converting numbers into neutral, positive and negative classes for the output of the bing dictionary
computer_531 <- computer_531 %>%
  mutate(sentiment_bing1 = "neutral")

for(i in 1:nrow(computer_531)){ 
  if(computer_531[i,]$sentiment_bing >= 1){
    computer_531[i,]$sentiment_bing1 <- "positive"
    } else if(computer_531[i,]$sentiment_bing <= -1){
    computer_531[i,]$sentiment_bing1 <- "negative"
    }
}

# performance of bing
confusionMatrix(table(computer_531$sentiment, computer_531$sentiment_bing1, dnn=c("Actual", "Predicted")))

```

---

15. **Create a confusion matrix, and calculate Accuracy, precision, recall and F1 measures for the output of the afinn dictionary.**

---

```{r 15, message = FALSE, warning = FALSE, include = TRUE}
# converting numbers into neutral, positive and negative classes for the output of the afinn dictionary
computer_531 <- computer_531 %>%
  mutate(sentiment_afinn1 = "neutral")
for(i in 1:nrow(computer_531)){ 
  if(computer_531[i,]$sentiment_afinn >= 1){
    computer_531[i,]$sentiment_afinn1 <- "positive"
    } else if(computer_531[i,]$sentiment_afinn <= -1){
    computer_531[i,]$sentiment_afinn1 <- "negative"
    }
}

# performance of afinn
confusionMatrix(table(computer_531$sentiment, computer_531$sentiment_afinn1, dnn=c("Actual", "Predicted")))

```

---

16. **From the `rpart` package, we want to build a tree on this dataset to predict sentiments. For this purpose, first prepare your data by doing preprocessing on reviews, converting to dtm and creating training and test sets.**

---

```{r 16, message = FALSE, warning = FALSE, include = TRUE}

set.seed(123)

corpus <- Corpus(VectorSource(computer_531$review))
# standardize to lowercase
corpus <- tm_map(corpus, content_transformer(tolower))
# remove tm stopwords
corpus <- tm_map(corpus, removeWords, stopwords())
# standardize whitespaces
corpus <- tm_map(corpus, stripWhitespace)
# remove punctuation
corpus <- tm_map(corpus, removePunctuation)
# remove numbers
corpus <- tm_map(corpus, removeNumbers)
dtm <- DocumentTermMatrix(corpus)
# words appearing more than 10x
features <- findFreqTerms(dtm, 10)
head(features)

train_idx <- createDataPartition(computer_531$sentiment, p=0.80, list=FALSE)
# set for the original raw data 
train1    <- computer_531[train_idx,]
test1     <- computer_531[-train_idx,]
# set for the cleaned-up data
train2    <- corpus[train_idx]
test2     <- corpus[-train_idx]


dtm_train <- DocumentTermMatrix(train2, list(dictionary=features))
dtm_test  <- DocumentTermMatrix(test2, list(dictionary=features))
dtm_train <- as.data.frame(as.matrix(dtm_train))
dtm_test  <- as.data.frame(as.matrix(dtm_test))

dtm_train1 <- cbind(cat=factor(train1$sentiment), dtm_train)
dtm_test1  <- cbind(cat=factor(test1$sentiment), dtm_test)
dtm_train1 <- as.data.frame(dtm_train1)
dtm_test1  <- as.data.frame(dtm_test1)

```

---

17. **Now build a tree with the default setting of the rpart function, and visualize the tree.**

---

```{r 17, message = FALSE, warning = FALSE, include = TRUE}
# build a small tree for prediction
fit_tree <- rpart(cat ~ ., data = dtm_train1, method = "class")
rpart.plot(fit_tree)

```

---

18. **Check the performamce of the tree on your training set.**

---

```{r 18, message = FALSE, warning = FALSE, include = TRUE}
# performance on train
preds <- predict(fit_tree, type = c("class"))
confusionMatrix(table(dtm_train1$cat, preds, dnn=c("Actual", "Predicted")))

```

---

19. **Check the performamce of the tree on the test set, and compare it with the training performance.**

---

```{r 19, message = FALSE, warning = FALSE, include = TRUE}
# performance on test
preds <- predict(fit_tree, newdata = dtm_test1, type = c("class"))
confusionMatrix(table(dtm_test1$cat, preds, dnn=c("Actual", "Predicted")))

```

Here you found that while decision trees could outperform a simple dictionary-based method, they are not very good with high dimensional data such as text. If you have some time left try a Random Forest, naive Bayes, or SVM to compare the performance. They will perform better, but you will lose interpretability!

---

# Summary

---

In this practical, we learned about:

- Sentiment analysis
- Dictionary-based methods
- Available sentiment dictionaries
- Learning sentiments
- Decision trees

---

End of Practical

---