---
title: "Text Representation and Classification"
author: "Ayoub Bagheri"
output:
  ioslides_presentation:
    logo: logo.png
    smaller: yes
    widescreen: no
  beamer_presentation: default
---
```{r, include=FALSE}
library(knitr)
library(DiagrammeR)
```

## Lecture’s Plan

- How to represent a document?

- What are vector space and bag-of-words models?

<!-- 3. Features in text? And how to do text feature selection? -->

- How to classify text data?

- How to evaluate a classifier?

# Text Classification

## Text classification

- Supervised learning: Learning a function that maps an input to an output based on example input-output pairs.
  
  - infer a function from labeled training data
  
  - use the inferred function to label new instances

- Human experts annotate a set of text data
  
  - Training set

```{r, echo=FALSE, out.width="40%", fig.align='center'}
include_graphics("img/page 24.png")
```

<!-- ## Applications -->

<!-- - Assigning subject categories, topics, or genres -->

<!-- - Spam detection -->

<!-- - Authorship identification -->

<!-- - Age/gender identification -->

<!-- - Language Identification -->

<!-- - Sentiment analysis -->

<!-- - … -->

## Text classification?

- Which problem is not (or less likely to be) a text classification task? 

  - Author's gender detection from text
  
  - Finding about the smoking conditions (yes/no) of patients from clinical letters

  - Grouping similar news articles

  - Classifying reviews into positive and negative sentiments

<!-- <br> -->
<!-- <br> -->
<!-- Go to <a href="www.menti.com">www.menti.com</a> and use the code 9594 3321 -->

## Pipeline
```{r, echo=FALSE}
grViz("digraph {

graph [layout = dot, rankdir = LR]

node [shape = rectangle, style = filled, fillcolor = Linen]

text [label = 'Text Collection', shape = folder, fillcolor = Beige]

repre [label =  'Text \n Representation']
class [label = 'Classification \n (Model Training)']
pred [label= 'Prediction \n (Test Data)']

# edge definitions with the node IDs
text  -> repre -> class -> pred
}")
```

# Text Representation

## How to represent a document

- Represent by a string?
  
  - No semantic meaning

- Represent by a list of sentences?

  - Sentence is just like a short document (recursive definition)

<!-- - Represent by a vector? -->

<!--   - A vector is an ordered finite list of numbers. -->

## Vector space model

- A vector space is a collection of vectors

- A vector is an ordered finite list of numbers.

- Represent documents by <u>concept</u> vectors
  
  - Each concept defines one dimension

  - A large number of concepts define a high-dimensional space
  
  - Element of vector corresponds to concept weight	
  
  - The process of converting text into numbers is called <b>Vectorization</b>

- Distance between the vectors in this concept space

  - Relationship among documents


## Vector space model

- Terms are generic features that can be extracted from text

- Typically, terms are single words, keywords, n-grams, or phrases

- Documents are represented as vectors of terms

- Each dimension (concept) corresponds to a separate term

$$d = (w_1, ..., w_n)$$

<!-- \newpage -->

<!-- ## Vector space model -->

<!-- - The Corpus represents a collection of documents (the dataset) -->

<!-- - The Vocabulary is the set of all unique terms in the corpus -->

## An illustration of VS model 

- All documents are projected into this concept space

```{r,echo=FALSE, out.width="70%"}
include_graphics("img/page 8.png")
```


## Vector space model

- Bag of Words, a Vector Space Model where:
  
  - Terms: words (more generally we may use n-grams, etc.)
  
  - Weights: number of occurrences of the terms in the document

- Topics (later)

- Word Embeddings (later)


## Bag of Words (BOW)

- Words (terms) and weights as the basis for vector representations of text

  - Doc1: Text mining is to identify useful information.
  
  - Doc2: Useful information is mined from text.
  
  - Doc3: Apple is delicious.

```{r, echo= FALSE, out.width="100%"}
include_graphics("img/page 12.png")
```


## BOW weights: Binary

- <b><span style="font-size:20px">Binary</span></b>

  - with 1 indicating that a term occurred in the document, and 0 indicating that it did not

## BOW weights: Raw term frequency

- Idea: a term is more important if it occurs more frequently in a document

- use the raw frequency count of term $t$ in doc $d$

<!-- - TF Formulas -->

<!--   - Let $t(c,d)$ be the frequency count of term $t$ in doc $d$ -->

<!--   - Raw TF: $tf(t,d) = c(t,d)$ -->

## BOW weights: TF-IDF
- Idea: a term is more discriminative if it occurs a lot but only in fewer documents. 

- TF-IDF (term frequency–inverse document frequency) weight: 

$$w_{d,t} = TF_{d,t} \cdot IDF_t$$

Let $n_{d,t}$ denote the number of times term $t$ appears in document $d$. The relative frequency of $t$ in $d$ is: 

$$TF_{d,t} = \frac{n_{d,t}}{\sum_i{n_{d,i}}}$$
Let $N$ denote the number of documents annd $N_t$ denote the number of documents containing term $t$.

$$IDF_t = log(\frac{N}{N_t})$$


<!-- ## Zipf’s law | A statistical property of language -->

<!-- - Zipf’s law -->

<!--   - Frequency of any word is inversely proportional to its rank in the frequency table -->
<!--   - Formally -->
<!--     - $f(k;s,N)=\frac{1/k^S}{\sum^N_{n=1}{1/n^S}}$ <br> -->
<!--     where $k$ is rank of the word; $N$ is the vocabulary size; $s$ is language-specific parameter -->
<!--   - Simply: $f(l;s,N)\propto1/k^S$ -->

<!-- ## Zipf’s law -->

<!-- ```{r, echo=FALSE, out.width="60%"} -->
<!-- include_graphics("img/page 14.png") -->
<!-- ``` -->

<!-- <br> -->
<!-- <p style="float: right;font-size:12px">Source:https://en.wikipedia.org/wiki/Zipf's_law</p> -->
  
<!-- ## Zipf’s law -->

<!-- <!-- - The most frequent word occurs 90,000 times, the second one 45,000 times, the third 30,000, and so on. --> 

<!-- - In a clinical text dataset, if we know the most popular word’s frequency is 69,542, what is your best estimate of its second and third most popular word’s frequencies respectively?  -->

<!-- Zipf’s law tells us: -->

<!-- - Head words take large portion of occurrences, but they are semantically meaningless -->
<!-- E.g., the, a, an, we, do, to -->
<!-- - Tail words take major portion of vocabulary, but they rarely occur in documents -->
<!-- E.g., Pneumonoultramicroscopicsilicovolcanoconiosis (45 letters, a lung disease) -->
<!-- - The rest is most representative -->



<!-- ## Similarity metric -->

<!-- - Euclidean distance -->

<!--   - $dist(d_i, d_j) = \sqrt{\sum_{t\in V}{[tf(t,d_i)idf(t) - tf(t, d_j)idf(t)]^2}}$ -->

<!--   - Longer documents will be penalized by the extra words -->

<!--   - We care more about how these two vectors are overlapped -->

<!-- ## In R -->

<!-- ```{r} -->
<!-- library(tm) -->
<!-- data <- c('Text mining is one of the Utrecht summer school courses.',  -->
<!--           'There are other data science courses in Utrecht summer school') -->

<!-- # convert data to vector space model -->
<!-- corpus <- VCorpus(VectorSource(data)) -->

<!-- # create a dtm object -->
<!-- dtm <- DocumentTermMatrix(corpus,  -->
<!--                           list(removePunctuation = TRUE,  -->
<!--                                stopwords = TRUE,  -->
<!--                                stemming = TRUE,  -->
<!--                                removeNumbers = TRUE)) -->
<!-- ``` -->


<!-- ## In R -->
<!-- ```{r} -->
<!-- inspect(dtm) -->
<!-- ``` -->


<!-- # Feature Selection -->

<!-- ## Feature selection for text classification -->

<!-- - Feature selection is the process of selecting a specific subset of the terms of the training set and using only them in the classification algorithm. -->

<!-- - high dimensionality of text features -->

<!-- - Select the most informative features for model training -->

<!--   - Reduce noise in feature representation -->

<!--   - Improve final classification performance -->

<!--   - Improve training/testing efficiency -->

<!--     - Less time complexity -->

<!--     - Fewer training data -->

<!-- ## Feature selection methods -->

<!-- - Wrapper methods -->
<!--   - Find the best subset of features for a particular classification method -->
<!--   - Sequential forward selection or genetic search to speed up the search -->

<!-- - Filter methods -->
<!--   - Evaluate the features <u>independently</u> from the classifier and other features -->
<!--   - Feasible for very large feature se -->
<!--   - Usually used as a preprocessing step -->

<!-- - Embedded methods -->
<!--  - e.g. Regularized regression, Regularized SVM -->

<!-- ```{r,echo=FALSE, out.width="70%", fig.align='center'} -->
<!-- include_graphics("img/page 34.png") -->
<!-- ``` -->

<!-- ## Feature selection methods -->

<!-- - Filter method -->

<!--   - Evaluate the features <u>independently</u> from the classifier and other features -->

<!--     - No indication of a classifier’s performance on the selected features -->

<!--     - No dependency among the features -->

<!--   - Feasible for very large feature set -->

<!--     - Usually used as a preprocessing step -->

<!-- ```{r,echo=FALSE, out.width="90%", fig.align='center'} -->
<!-- include_graphics("img/page 37.png") -->
<!-- ``` -->

<!-- ## Document frequency -->

<!-- - Rare words: non-influential for global prediction, reduce vocabulary size -->

<!-- ```{r,echo=FALSE, out.width="90%", fig.align='center'} -->
<!-- include_graphics("img/page 38.png") -->
<!-- ``` -->

<!-- ## Information gain -->

<!-- - Decrease in entropy of categorical prediction when the feature is present or absent -->

<!-- ```{r,echo=FALSE, out.width="90%", fig.align='center'} -->
<!-- include_graphics("img/page 39.png") -->
<!-- ``` -->

<!-- ## Gini Index -->

<!-- Let $p(c | t)$ be the conditional probability that a document belongs to class $c$, given the fact that it contains the term $t$. Therefore, we have: -->

<!-- $$\sum^k_{c=1}{p(c | t)=1}$$ -->

<!-- Then, the gini-index for the term $t$, denoted by $G(t)$ is defined as: -->

<!-- $$G(t) = \sum^k_{c=1}{p(c | t)^2}$$ -->

<!-- ## Gini Index -->

<!-- - The value of the gini-index lies in the range $(1/k, 1)$.  -->

<!-- - Higher values of the gini-index indicate a greater discriminative power of the term t.  -->

<!-- - If the global class distribution is skewed, the gini-index may not accurately reflect the discriminative power of the underlying attributes. -->

<!-- - Other methods -->
<!--   - Normalized gini-index -->
<!--   - Mutual Information -->
<!--   - ${\chi}^2$-Statistic -->

# Classification Algorithms

## How to classify this document? 

```{r, echo=FALSE, out.width="100%"}
include_graphics("img/page 53.png")
```

## Text classification: Definition

- Supervised learning

- Input:
 
  - A training set of $m$ manually-labeled documents $(d_1,c_1),\cdots,(d_m,c_m)$
  
  - A fixed set of classes $C = \{c_1, c_2,…, c_J\}$
  
- Output:

  - A learned classifier $y:d \rightarrow c$
  
## Hand-coded rules

- Rules based on combinations of words or other features

- Rules carefully refined by expert

- But building and maintaining these rules is expensive

- Data/Domain specifics

- Not recommended!

## Supervised machine learning
- Nearest centroid
- K-nearest neighbors
- Naïve Bayes
- Decision tree
- Random forest
- Support vector machines
- Logistic regression
- Neural networks (deep learning)

## Rocchio classifier (Nearest Centroid)

Each class is represented by its centroid, with test samples classified to the class with the nearest centroid. Using a training set of documents, the Rocchio algorithm builds a prototype vector, centroid, for each class. This prototype is an average vector over the training documents’ vectors that belong to a certain class. 

$$\boldsymbol{\mu_c} = \frac{1}{|D_c|}\sum_{\mathbf{d} \in D_c}{\mathbf{d}}$$

Where $D_c$ is the set of documents in the corpus that belongs to class $c$ and $d$ is the vector representation of document $d$.

## Rocchio Classifier (Nearest Centroid)

The predicted label of document d is the one with the smallest (Euclidean) distance between the document and the centroid.

$$\hat{c} = \arg \min_c ||\boldsymbol{\mu_c} - \mathbf{d}||$$

```{r, echo=FALSE, out.width="70%", fig.align='center'}
include_graphics("img/page 53a.png")
```


## K-Nearest Neighbor (KNN)

```{r, echo=FALSE, out.width="80%", fig.align='center'}
include_graphics("img/page 60.png")
```

## KNN

- Given a test document $d$, the KNN algorithm finds the $k$ nearest neighbors of $d$ among all the documents in the training set, and scores the category candidates based on the class of the k neighbors. 

- After sorting the score values, the algorithm assigns the candidate to the class with the highest score. 

- The basic nearest neighbors classification uses uniform weights: that is, the value assigned to a query point is computed from a simple majority vote of the nearest neighbors. 

- Can weight the neighbors such that nearer neighbors contribute more to the fit. 


## Naïve Bayes

```{r, echo=FALSE, out.width="100%"}
include_graphics("img/page 62.png")
```

## Bayes rule 

- Applied to documents and classes

- For a document $d$ and a class $c$

$$P(c|d) = \frac{P(c)P(d|c)}{P(d)}$$

## Multinomial naïve Bayes assumptions

$$C_{NB} = \underset{c \in C}{\operatorname{argmax}}P(c) \cdot P(w_1, w_2, \ldots,w_n|c)$$

- <b>Bag of Words assumption</b>: Assume position doesn’t matter

- <b>Conditional Independence</b>: Assume the feature probabilities $P(w_i|c)$ are independent given the class $c$.

$$P(w_1, \ldots, w_n|c) = P(w_1 | c) \cdot P(w_2|c) \cdot P(w_3|c) \cdot \ldots \cdot P(w_n|c)$$

- Hence:

$$C_{NB} = \underset{c \in C}{\operatorname{argmax}}P(c) \cdot P(w_1 | c) \cdot P(w_2|c) \cdot P(w_3|c) \cdot \ldots \cdot P(w_n|c)  $$
$$C_{NB} = \underset{c \in C}{\operatorname{argmax}}P(c)\prod_{i \in positions}{P(w_i|c)}$$

<!-- \newpage  -->

<!-- ## Multinomial Naïve Bayes Classifier -->

<!-- $$C_{MAP} = \underset{c \in C}{\operatorname{argmax}}P(w_1, w_2, \ldots,w_n|c)P(c)$$ -->

<!-- $$C_{NB} = \underset{c \in C}{\operatorname{argmax}}P(c_j)\prod_{w \in V}{P(w|c)}$$ -->
<!-- $$C_{NB} = \underset{c \in C}{\operatorname{argmax}}P(c_j)\prod_{i \in positions}{P(w_i|c_i)}$$ -->
\newpage

## Parameter estimation

- First attempt: maximum likelihood estimates

  - simply use the frequencies in the data

$$\hat{P}(c) = \frac{count(C = c)}{N_{doc}}$$
$$\hat{P}(w_i|c) = \frac{count(w_i, c)}{\sum_{w \in V}count(w, c)}$$

<!-- - fraction of times word $w_i$ appears among all words in documents of class $c$ -->

\newpage

## Problem with Maximum Likelihood

What if we have seen no training documents with the word <b>coffee</b>  and classified in the topic <b>positive (thumbs-up)</b>?

$$\hat{P}(\mathrm{''coffee''|positive}) = \frac{count(\mathrm{''coffee'', positive})}{\sum_{w \in V}{count(\mathrm{w,positive})}}$$

Zero probabilities cannot be conditioned away, no matter the other evidence!

$$C_{NB} = \underset{c \in C}{\operatorname{argmax}}P(c)\prod_{i \in positions}{P(w_i|c)}$$
\newpage

## Laplace (add-1) smoothing for naïve Bayes
<!-- $$ -->
<!-- \begin{align} -->
<!-- \hat{P}(w_i|c) &= \frac{count(w_i,c)+1}{\sum_{w \in V}{(count(w,c)+1)}} \\ -->
<!-- &= \frac{count(w_i,c)+1}{(\sum_{w \in V}{count(w,c) + |V|})} -->
<!-- \end{align} -->
<!-- $$ -->
$$
\hat{P}(w_i|c) = \frac{count(w_i,c)+1}{\sum_{w \in V}{(count(w,c)+1)}}
$$

\newpage

<!-- ## Multinomial Naïve Bayes: Learning -->
<!-- - From training corpus, extract <em>Vocabulary</em> -->

<!-- <div style="float:left;width:50%"> -->
<!-- - Calculate $P(c_j)$ terms -->
<!--   - For each $c_j$ in $C$ do -->

<!--   $docs_j\leftarrow$ all docs with class = $c_j$ -->

<!--   $$P(c_j) \leftarrow \frac{|docs_j|}{|total\ \#\ documents|}$$ -->
<!-- </div> -->

<!-- <div style="float:right;width:50%"> -->
<!-- - Calculate $P(w_k|c_j)$ terms</p> -->
<!--   - $Text_j \leftarrow$ single doc containing all $docs_j$ -->

<!--   - For each word $w_k$ in <em>Vocabulary</em> -->

<!--     $n_k \leftarrow$ # of occurrences of $w_k$ in $Text_j$ -->

<!--   $$P(w_k|c_j) \leftarrow \frac{n_k + \alpha}{n + \alpha|Vocabulary|}$$ -->
<!-- </div> -->

\newpage

## Decision tree 

```{r, echo=FALSE, out.width="55%", fig.align='center'}
include_graphics("img/decisionTree.png")
```

- A decision tree is a hierarchical decomposition of the (training) data space, where a condition on the feature value is used to divide the data space hierarchically. 

- Top-down, by choosing a variable at each step that best splits the set of items. 
- Different algorithms to measure the homogeneity of the target variable within the subsets (e.g. Gini impurity, information gain)

\newpage

## Random forest

- Random forests are an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time.
- Fit multiple trees to bootstrapped samples of the data AND at each node select best predictor from only a random subset of predictors. Combine all trees to yield a consensus prediction


```{r, echo=FALSE, out.width="55%", fig.align='center'}
include_graphics("img/rf.png")
```

## Support Vector Machine (SVM)

- The main principle of SVM is to determine separators in the search space which can best separate the different classes.
- SVM tries to make a decision boundary in such a way that the separation between the two classes is as wide as possible.

```{r, echo=FALSE, out.width="50%", fig.align='right'}
include_graphics("img/page 74.png")
```

\newpage

## SVM

- It is not necessary to use a linear function for the SVM classifier. 
- With the kernel trick, SVM can construct a nonlinear decision surface in the original feature space by mapping the data instances non-linearly to a new space where the classes can be separated linearly with a hyperplane. 

<!-- - In practice, linear SVM is used most often because of their simplicity and ease of interpretability. -->
- SVM is quite robust to high dimensionality.

```{r, echo=FALSE, out.width="50%", fig.align='right'}
include_graphics("img/page 75.png")
```

\newpage


# Evaluation

## Data Splitting

- Training set

  - Validation set (dev set)
    - A dataset of examples used to tune the hyperparameters (i.e. the architecture) of a classifier. It is sometimes also called the development set or the "dev set".
    
- Test set

## K-fold cross validation

```{r, echo=FALSE, out.width="75%"}
include_graphics("img/page 31.png")
```

<br>
<p style="font-size:12px">adapted from https://scikit-learn.org/stable/modules/cross_validation.html</p>


## Confusion matrix

```{r, echo=FALSE, out.width="100%"}
include_graphics("img/page 81.png")
```

\newpage

## Accuracy

- What proportion of instances is correctly classified? <br>
 	<center>TP + TN / TP + FP + FN + TN </center>

- Accuracy is a valid choice of evaluation for classification problems which are well balanced and not skewed. 

- Let us say that our target class is very sparse. Do we want accuracy as a metric of our model performance? What if we are predicting if an asteroid will hit the earth? Just say “No” all the time. And you will be 99% accurate. The model can be reasonably accurate, but not at all valuable.

\newpage

## Precision and recall

- <b>Precision</b> (also Positive Predictive Value): % of selected/retrieved items that are correct/relevant <br> 

- <b>Recall</b> (also sensitivity): % of correct/relevant items that are selected/retrieved.

```{r, echo=FALSE, out.width="50%", fig.align='center'}
include_graphics("img/precisionRecall.png")
```

<!-- <p style="font-size:5px">https://en.wikipedia.org/wiki/Precision_and_recall</p> -->

- Precision is a valid choice of evaluation metric when we want to be very sure of our prediction.

- Recall  is a valid choice of evaluation metric when we want to capture as many positives as possible.

\newpage

## A combined measure: F

A combined measure that assesses the precision/recall tradeoff is F measure (weighted harmonic mean):

<!-- $$F = \frac{1}{\alpha\frac{1}{P}+(1-\alpha)\frac{1}{R}}=\frac{(\beta^2+1)PR}{\beta^2P + R}$$ -->
$$F = \frac{(\beta^2+1)PR}{\beta^2P + R}$$
where $\beta$ is a positive real number and is chosen such that recall is considered $\beta$ times as important as precision. 
<!-- The harmonic mean is a very conservative average; -->

Balanced F1 measure: $\beta = 1$, $F = 2PR/(P+R)$
<!-- - $\beta = 1$ (that is, $\alpha = 1/2$): $F = 2PR/(P+R)$ -->


# The Real World

## No training data? 

1. Manually written rules

  - <p style="color:green">If (x or y) and not (w or z) then categorize as class1</p>

  - Need careful crafting

  - Low accuracy

  - Domain-specific

  - Time-consuming

2. Active learning

3. Unsupervised methods (tomorrow)

\newpage

## Very little data?

- Use Naïve Bayes, KNN, Rocchio
  
  <!-- - Naïve Bayes is a “high-bias” algorithm <span style="color:green; font-size:12px">(Ng and Jordan 2002 NIPS)</span> -->

- Get more labeled data 
  
- Find ways to label data

- Try semi-supervised methods

- Try transfer learning
  
  <!-- - e.g. active learning, bootstrapping -->

\newpage

## A reasonable amount of data?

- Works with the more complex classifiers
  
  - SVM
  
  <!-- - Regularized Logistic Regression -->

  - Random forest
  
  - Neural network

\newpage

## A huge amount of data?

- Can achieve high accuracy!

- At a cost:

  - SVM 
  - NN, deep learning (train time)

  <!-- - Regularized logistic regression -->
  <!-- - Naïve Bayes again! -->
  <!-- - Deep learning -->

\newpage

## Accuracy as a function of data size
<div style="float:left; width:50%">
- With enough data
  
  - Classifier may not matter
</div>
<div style="float:right; width:50%">
```{r,echo=FALSE, out.width="100%"}
include_graphics("img/page 92.png")
```
</div>

https://aclanthology.org/P01-1005.pdf

\newpage

## How to tweak performance
- Domain-specific features and weights: very important in real performance

- Sometimes need to collapse terms:
  
  - Part numbers, chemical formulas, …
  
  - But stemming generally doesn’t help

- Upweighting: Counting a word as if it occurred twice:
  - Title words

  - First sentence of each paragraph (Murata, 1999)
  
  - In sentences that contain title words
  
- Hyperparameter optimization

<!-- # Terminology -->

<!-- ## Some terminology {.smaller} -->
<!-- - <p style="font-size:16px"><u><b>Corpus</b></u>: is a large and structured set of texts</p> -->
<!-- - <p style="font-size:16px"><u><b>Stop words</b></u>:  words which are filtered out before or after processing of natural language data (text)</p> -->
<!-- - <p style="font-size:16px"><u><b>Unstructured text</b></u>: information that either does not have a pre-defined data model or is not organized in a pre-defined manner.</p> -->
<!-- - <p style="font-size:16px"><u><b>Tokenizing</b></u>: process of breaking a stream of text up into words, phrases, symbols, or other meaningful elements called tokens (see also lexical analysis)</p> -->
<!-- - <p style="font-size:16px"><u><b>Natural language processing</b></u>: field of computer science, artificial intelligence, and linguistics concerned with the interactions between computers and human (natural) languages</p> -->
<!-- - <p style="font-size:16px"><u><b>Term document (or document term) matrix</b></u>:  is a mathematical matrix that describes the frequency of terms that occur in a collection of documents</p> -->
<!-- - <p style="font-size:16px"><u><b>Supervised learning</b></u>: is the machine learning task of inferring a function from labeled training data</p> -->
<!-- - <p style="font-size:16px"><u><b>Unsupervised learning</b></u>: find hidden structure in unlabeled data</p> -->

\newpage

# Summary

## Summary

- Vector space model & BOW

<!-- - Feature Selection -->

- Text classification --> supervised learning --> labeled data

- Evaluation

# Practical 3
