---
title: "Practical 2: Processing Textual Data in R"
author: "Luka van der Plas, Mees van Stiphout, Jos√© de Kruif"
date: "Introduction to Text Mining with R"
#date: "12/07/2021"
mainfont: Arial
fontsize: 12pt
urlcolor: blue
output: 
  html_document:
    code_folding: hide
    toc: true
    toc_depth: 1
    toc_float: true
    df_print: paged
    theme: paper
    pandoc_args: --output=Practical_2.html
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, messsage=FALSE, warning=FALSE)
```

# Introduction

Welcome to the second practical of the course "Introduction to Text Mining with R". 

In this practical, we will work with text data, cleaning tools and try some visualizations.

As always we start with the packages we are going to use. Be sure to run these lines in your session to load the proper packages before you continue. If there are packages that you have not yet installed, first install them with `install.packages().

```{r load_packages}
library(tidyverse) # for data manipulation
library(dplyr)     # for data manipulation
library(ggplot2)   # for visualization
library (SnowballC) # for stemming
library(tokenizers) # for tokenisation
library(tidytext)  # for counting words
library(wordcloud) # to create pretty word clouds
```

Before starting the exercises, you need to set your working directory to your Practicals folder. To this end, you can either create a project in RStudio and move the Rmd and data files to the project folder, or you can use the following line instead of creating a new project:


**Do not forget to adjust your working directory to the right folder.**

```{r set_wd}
# setwd("path/to/your/data")
```


# Reading  data: CSV files

In this practical, we will be working with a dataset of book reviews from Goodreads.
You can download the data from https://ayoubbagheri.nl/r_tm/#monday.

The data consists of two files:
- `goodreads_english.csv` contains only reviews in English
- `goodreads_mixed.csv` contains reviews in various languages

For now, we will be working with the English reviews.

1. **Use the `read.csv function to import the `goodreads_english.csv` and `goodreads_mixed.csv` files**

```{r import_data}
goodreads_english <- read.csv("goodreads_english.csv", stringsAsFactors=FALSE)
goodreads_mixed <- read.csv("goodreads_mixed.csv", stringsAsFactors=FALSE)
```


We can inspect the first few lines to see what the data looks like.

```{r show_data_head}
head(goodreads_english)
```

How many rows do we have?

```{r show_data_nrow}
nrow(goodreads_english)
```

---

# Cleaning

We will go through a few common steps in text cleaning, where we make basic replacements in a text.

Let's take one review as an example and see what we can do.

```{r pick_example_text}
example_text = goodreads_english[1, 'text']

example_text
```

## All words in lowercase

```{r convert_lowercase}
tolower(example_text)
```

## Clean up whitespace

The string below includes a double space, as well as a few newlines.

```{r make_some_text}
some_text <- paste("I loved this book!", "It was  great.", "Looking forward  to the sequel.", sep="\n")
cat(some_text)
```

What's wrong with this? When we have a "clean" string, we can easily split the text into words by splitting on every space `" "`:

```{r clean_split_demo}
strsplit("I loved this book!", " ")
```

But with a messy string, this gets tricky:

```{r messy_split_demo}
strsplit(some_text, " ")
```

Let's try to clean that up. The function `gsub` is the most basic way to make replacements in a string. 

```{r gsub_demo}
gsub("bad", "good", "it was a bad book")
```

To remove something, we can just replace it with nothing:

```{r gsub_demo_replace_with_nothing}
gsub("not", "", "I did not like the book")
```

**Excercise 1:** try to clean up `some_text`: it should have no double spaces or newlines. To see if it works, try using `strsplit` on your result.

```{r exc_1}
```

## Remove numbers and punctuation

Removing numbers or punctuation is also a kind of replacement. To replace *all the numbers*, we could write some code like so: 

```r
replaced <- example_text
replaced <- gsub("0", "", replaced)
replaced <-g sub("1", "", replaced)
replaced <-g sub("2", "", replaced)
# etc...
```

But this is way too much typing. Luckily, `gsub` also takes regular expressions, which we saw in the previous practical. If you have trouble understanding the expressions below, don't worry too much about it. For now, the important thing is that regular expressions *can* be used to describe patterns like "all the numbers" or "periods, commas and hyphens".

```{r remove_numbers}
gsub("[0-9]", "", example_text)
```

```{r remove_punct}
gsub("[\\.,\\-]", "", example_text)
```

## Putting it together

**Exercise 2:** combine all the steps we've used so far on `example_text`

```{r exc_2}

```

As you can see, there are quite a few steps involved! However, these steps are very common, and as we will see, there are packages that make this easy for us.

# Tokenising

We already saw `strsplit` as a way to split a text into words, but that function is quite basic. From now on, we will use the `tokenizers` package to split our text into words. Let's try running it with its default options:

```{r tokenize_example_text}
example_words <- tokenize_words(example_text)

example_words
```

As you can see, `tokenize_words` does a bit more than just tokenisation. For example, all the text is in lowercase now. Try running `help(tokenize_words)` to see all the options that the functions offers, and adjust `example_words` so it does not include `"1986"` and `"2008"`.

# Stemming

The `SnowballC` pacakage allows us to stem words:

```{r stemming_demo}
wordStem(c("text", "texts", "texting"))
```

The `tokenize_words` function also has a handy variant, `tokenize_word_stems`, which uses the `snowballC` stemmer:

```{r stem_and_tokenize_example_text}
tokenize_word_stems(example_text)
```

## Stemming in other languages

`tokenize_word_stems` is based on English by default. We can get an overview with:

```{r show_stemming_langs}
getStemLanguages()
```

**Exercise 3:** Use `tokenize_word_stems` on some Dutch text. Use `help` to see how you can adjust the language of the stemmer.

```{r make_dutch_example}
example_dutch <- goodreads_mixed[14, "text"]
```

# Making a bag of words

We have only worked on an  single review so far, but we want to work on our entire dataset. We will use `tidytext` to work with our data. Here we reshape our `goodreads_english` dataframe: each review is split over multiple rows, so we only have one word per row.

```{r make_token_df}
goodreads_by_word <- goodreads_english %>%
  unnest_tokens(word, text)

head(goodreads_by_word)
```

Now we can use `count` to get the most common elements in the `word` column:

```{r count_tokens}
goodreads_counts = goodreads_by_word %>%
  count(word, sort=TRUE)

goodreads_counts
```

Let's try visualising our results in a wordcloud.

```{r make_wordcloud}
goodreads_counts %>%
  with(wordcloud(word, n, max.words=100))
```

How many words are there in this dataset anyway? In text mining, we differentiate between *types* and *tokens*. The number of types tells us how many different words occur, and the number of tokens tells us how many words occur in total. For example, *"to be or not to be"* has 4 types (*to*, *be*, *or*, and *not*), and 6 tokens (*to*, *be*, *or*, *not*, *to*, and *be*).

**Exercise 4:** How many types are in `goodreads_counts`? And how many tokens?

```{r exc_4}
```

**Exercise 5:** The `unnest_tokens` that we used above takes the same arguments as `tokenize_words`: you can tweak whether you want to turn everything into lowercase and whether to clean up punctuation or numbers. Create a new table where you adjust one of these options, and report on how it affects the number of types and tokens.

```{r exc_5}
```

# Remove stopwords

The wordcloud we made in the previous section is nice, but it contains a lot of generic words like "the", "and", etc. Let's try removing stopwords.

We will use `get_stopwords` from the `tidytext` package to get a list of stopwords in English.

```{r get_stopwords}
stopwords_english <- get_stopwords()
stopwords_english
```

Now, we want to exclude these from our table of words. We can use `anti_join`, which means that we cross-reference with another table, and remove everything that is also in the other table. Then, we use `count` again to get a new frequency table.

```{r exclude_stopwords}
goodreads_without_stopwords <- goodreads_by_word %>%
  anti_join(stopwords_english, by='word') %>%
  count(word, sort=TRUE)

head(goodreads_without_stopwords)
```

This already looks a lot more informative: the most frequent words include *book*, *read* *story*, and *characters*: they tell us something about what this dataset is about. We can make another wordcloud that does not include the stopwords:

```{r show_cleaned_wordcloud}
goodreads_without_stopwords %>%
  with(wordcloud(word, n, max.words=100))
```

**Exercise 6:** We can use `filter`to select the reviews from a specific book, like so. Make a word cloud for the reviews of *Nineteen eighty-four* and *Gone girl* and compare them side-by-side. Do they look very different?

```{r filter_title_demo}
goodreads_english %>%
  filter(book_title == "Nineteen eighty-four")
```

# POS tagging

`tidytext` provides a table `parts_of_speech` which lists potential parts of speech for English words. We can use this for a very basic way of assigning a POS to each word, by looking up each word in the table. Here we do just that: `left_join` means that for each row in `goodreads_by_word`, we look up the corresponding row in `parts_of_speech` and combine the data.

```{r add_pos_tags}
goodreads_pos <- goodreads_by_word %>%
  left_join(parts_of_speech, by="word", multiple="first")

head(goodreads_pos)
```

Be aware that this is a very crude way of assigning POS tags. We usually want to take the rest of the sentence into account, so we can differentiate between two sentences like:

- _I want to **read** this book._
- _This book is a great **read**._

You can see this going wrong in the table above: "read" is classified as a noun instead of a verb. However, we can accept a some inaccuracy for our purposes here.

**Exercise 7:** Try using `filter` to only get a table containing only adjectives. Then filter on the rating so you can compare the adjectives used in the highest ratings (5) with those used in the lowest ratings (1). Make a wordcloud for both.

```{r exc_7}
```

# Mixed-language data

`goodreads_mixed` contains reviews in a mix of languages. We can see how many reviews we have for each:

```{r show_mixed_langs}
goodreads_mixed %>%
  count(language, sort=TRUE)
```


**Exercise 8:** Filter the Spanish reviews (or pick another language, if you like). Go through the preprocessing steps we have covered so far and visualise the results. Think about which steps would need to be adjusted based on the language, and adjust them accordingly.

```{r exc_8}
```


