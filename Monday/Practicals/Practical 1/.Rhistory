?curve
dbeta(20,1)
?dbeta
runif(1, 0, 1)
#daniel is less than 0.5, dong is more than 0.5
set.seed(6161138)
runif(1, 0, 1)
set.seed(6161138)
runif(1, 0, 1)
set.seed(6161138)
runif(1, 0, 1)
set.seed(6161138)
runif(1, 0, 1)
set.seed(42)
runif(1, 0, 1)
round(runif(1, 0, 1))
set.seed(42)
round(runif(1, 0, 1))
set.seed(42)
round(runif(1, 0, 1))
set.seed(42)
round(runif(1, 0, 1))
set.seed(42)
round(runif(1, 0, 1))
set.seed(42)
round(runif(1, 0, 1))
set.seed(42)
round(runif(1, 0, 1))
set.seed(42)
round(runif(1, 0, 1))
set.seed(42)
round(runif(1, 0, 1))
set.seed(42)
round(runif(1, 0, 1))
set.seed(42)
round(runif(1, 0, 1))
set.seed(42)
round(runif(1, 0, 1))
set.seed(42)
round(runif(1, 0, 1))
set.seed(42)
round(runif(1, 0, 1))
set.seed(42)
round(runif(1, 0, 1))
set.seed(42)
round(runif(1, 0, 1))
set.seed(42)
round(runif(1, 0, 1))
set.seed(42)
round(runif(1, 0, 1))
set.seed(42)
round(runif(1, 0, 1))
set.seed(42)
round(runif(1, 0, 1))
set.seed(42)
round(runif(1, 0, 1))
library(tidyverse)
library(haven)
read_spss('C:/Users/6161138/Documents/GitHub/sentence_embeddings_for_surveys/data/ESS9e03.0_F1.sav')
ESS09 <- read_spss('C:/Users/6161138/Documents/GitHub/sentence_embeddings_for_surveys/data/ESS9e03.0_F1.sav')
ESS09 %>%
select(wkhtot)
ESS09 %>%
distinct(wkhtot)
ESS09 %>%
distinct(wkhtot) %>%
print(n=Inf)
ESS09$wkhtot %>% attr('labels')
ESS09 %>%
distinct(wkhtot) %>%
print(n=Inf)
ESS09$wkhtot %>% attr('labels')
ESS09$edubgb2 %>% attr('labels')
ESS09$edubgb2 %>% attr('labels')
ESS09_stata <- read_dta('C:/Users/6161138/Documents/GitHub/sentence_embeddings_for_surveys/data/ESS9e03.0_F1.dta')
ESS09 <- read_spss('C:/Users/6161138/Documents/GitHub/sentence_embeddings_for_surveys/data/ESS9e03.0_F1.por')
ESS09 %>%
distinct(wkhtot) %>%
print(n=Inf)
ESS09 <- read_sas('C:/Users/6161138/Documents/GitHub/sentence_embeddings_for_surveys/data/ESS9e03.0_F1.sas')
ESS09 <- read_sas('C:/Users/6161138/Documents/GitHub/sentence_embeddings_for_surveys/data/ESS9e03_0_F1.sas')
ESS09 <- read_sas('C:/Users/6161138/Documents/GitHub/sentence_embeddings_for_surveys/data/ESS9e03_0_F1.sas',
'C:/Users/6161138/Documents/GitHub/sentence_embeddings_for_surveys/data/ESS9e03_0_F1.txt')
ESS09 %>% attr('labels')
ESS09$edubgb2 %>% attr('labels')
typeof(ESS09$edubgb2 %>% attr('labels'))
label <- ESS09$edubgb2 %>% attr('labels')
print(label)
as.list(label)
typeof(label)
tibble(label)
unlist(label)
list(label)
as.list(label)
as.list(label)
as.list(label)
as.list(label)
exp(3.634)
knitr::opts_chunk$set(collapse = TRUE)
library(dplyr)     # for data manipulation
library(plyr)      # for data manipulation
library(magrittr)  # for pipes
library(tidyverse) # for tidy data and pipes
library(ggplot2)   # for visualization
library(read)      # for working with excel files
library(dplyr)     # for data manipulation
library(plyr)      # for data manipulation
library(magrittr)  # for pipes
library(tidyverse) # for tidy data and pipes
library(ggplot2)   # for visualization
library(xlsx)      # for working with excel files
library(tidyverse) # for tidy data and pipes
library(dplyr)     # for data manipulation
library(plyr)      # for data manipulation
library(magrittr)  # for pipes
library(tidyverse) # for tidy data and pipes
library(ggplot2)   # for visualization
#library(xlsx)      # for working with excel files
library(qdap)      # provides parsing tools for preparing transcript data
library(dplyr)     # for data manipulation
library(plyr)      # for data manipulation
library(magrittr)  # for pipes
library(tidyverse) # for tidy data and pipes
library(ggplot2)   # for visualization
#library(xlsx)      # for working with excel files
library(qdap)      # provides parsing tools for preparing transcript data
library(rJava)
Sys.getenv("R_ARCH")
library(dplyr)     # for data manipulation
library(plyr)      # for data manipulation
library(magrittr)  # for pipes
library(tidyverse) # for tidy data and pipes
library(ggplot2)   # for visualization
#library(xlsx)      # for working with excel files
library(qdap)      # provides parsing tools for preparing transcript data
library(wordcloud) # to create pretty word clouds
library(stringr)
library(dplyr)     # for data manipulation
library(plyr)      # for data manipulation
library(magrittr)  # for pipes
library(tidyverse) # for tidy data and pipes
library(ggplot2)   # for visualization
library(xlsx)      # for working with excel files
library(qdap)      # provides parsing tools for preparing transcript data
library(wordcloud) # to create pretty word clouds
library(stringr)
setwd("~/GitHub/r_tm/Monday/Practicals/Practical 1")
blog_gender_data <- read.xlsx("blog-gender-dataset.xlsx", 1)
blog_gender_1000 <- sample_n(blog_gender_data, 1000)
head(blog_gender_1000)
tail(blog_gender_1000)
# View(blog_gender_1000)
blog_gender_1000
#library(xlsx)      # for working with excel files
library(readxl)
blog_gender_data <- read_excel("blog-gender-dataset.xlsx", 1)
blog_gender_1000 <- sample_n(blog_gender_data, 1000)
blog_gender_1000
blog_gender_data <- read_excel("blog-gender-dataset.xlsx", 1) %>%
select(blog, gender)
blog_gender_1000 <- sample_n(blog_gender_data, 1000)
blog_gender_1000
tail(blog_gender_1000)
blog_gender_1000 %$% wordcloud(blog, min.freq = 20, max.words = 50, random.order = FALSE,
colors = brewer.pal(8, "Dark2"))
imdb_data <- read_csv("IMDB Dataset.csv")
head(blog_gender_1000)
tail(blog_gender_1000)
head(imdb_data)
imdb_500 <- ddply(imdb_data, "sentiment", .fun = function(x) {sample_n(x, size = 250)})
set.seed(123)
imdb_500 <- ddply(imdb_data, "sentiment", .fun = function(x) {sample_n(x, size = 250)})
blog_gender_1000 %$% wordcloud(blog, min.freq = 20, max.words = 50, random.order = FALSE,
colors = brewer.pal(8, "Dark2"))
blog_gender_1000 %$% wordcloud(blog, min.freq = 20, max.words = 50, random.order = FALSE,
colors = brewer.pal(8, "Dark2"))
?wordcloud
set.seed(123)
blog_gender_1000 %$% wordcloud(blog, min.freq = 20, max.words = 50, random.order = FALSE,
colors = brewer.pal(8, "Dark2"))
set.seed(123)
blog_gender_1000 %$% wordcloud(blog, min.freq = 20, max.words = 50, random.order = FALSE,
colors = brewer.pal(8, "Dark2"))
set.seed(123)
set.seed(123)
blog_gender_1000 <- sample_n(blog_gender_data, 1000)
set.seed(123)
imdb_500 <- ddply(imdb_data, "sentiment", .fun = function(x) {sample_n(x, size = 250)})
set.seed(123)
imdb_500 %>%
filter(sentiment == "positive") %$%
wordcloud(review, min.freq = 20,
max.words = 50, random.order = FALSE,
colors = brewer.pal(8, "Dark2"))
set.seed(123)
imdb_500 %>%
filter(sentiment == "negative") %$%
wordcloud(review, min.freq = 20,
max.words = 50, random.order = FALSE,
colors = brewer.pal(8, "Dark2"))
frequent_terms_pos <- imdb_500 %>%
filter(sentiment == "positive") %$%
freq_terms(review, top = 30)
frequent_terms_neg <- imdb_500 %>%
filter(sentiment == "negative") %$%
freq_terms(review, top = 30)
frequent_terms_pos
frequent_terms_neg
ggplot(frequent_terms_pos, aes(x = reorder(WORD, FREQ), y = FREQ)) +
geom_bar(stat = "identity") +
coord_flip()                +
xlab("Word in Corpus")      +
ylab("Count")               +
theme_minimal()
ggplot(frequent_terms_neg, aes(x = reorder(WORD, FREQ), y = FREQ)) +
geom_bar(stat = "identity") +
coord_flip()                +
xlab("Word in Corpus")      +
ylab("Count")               +
theme_minimal()
computer_data <- readLines("computer.txt")
computer_531 <- tibble(computer_data)
computer_531
computer_531 <- data.frame(computer_data)
head(computer_531)
str(computer_531)
computer_531 <- tibble(computer_data)
head(computer_531)
tail(computer_531)
# using regular expressions
rm(list = ls()[grep("data", ls())])
rm(list = ls()[grep("^fre", ls())])
# or
# rm(blog_gender_data, computer_data, frequent_terms_pos, frequent_terms_neg, imdb_data)
# or use except files
# rm(list=setdiff(ls(), c("computer_531", "imdb_500", "blog_gender_1000")))
computer_531
reviews_act <- grep("[Aa]ction", imdb_500$review)
reviews_com <- grep("[Cc]omedy", imdb_500$review)
reviews_dra <- grep("[Dd]rama", imdb_500$review)
genres    <- c("Action", "Comedy", "Drama")
nr_genres <- c(length(reviews_act), length(reviews_com), length(reviews_dra))
tibble(genres, nr_genres)
reviews_act
?grep
grep("Comedy", imdb_500$review, value = TRUE)
# you can also use the returned index instead of setting value
r <- regexpr('\\b[A-Z]+\\b', blog_gender_1000[1:20,]$blog)
r
regmatches(blog_gender_1000[1:20,]$blog, r)
r
?regexpr
?regmatches
str_extract(blog_gender_1000[1:20,]$blog, '\\b[A-Z]+\\b')
r <- regexpr('\\b[A-Z]+\\b', blog_gender_1000[1:20,]$blog)
r
regmatches(blog_gender_1000[1:20,]$blog, r)
r <- gregexpr('\\b[A-Z]+\\b', blog_gender_1000[1:10,]$blog)
regmatches(blog_gender_1000[1:10,]$blog, r)
str_match_all(blog_gender_1000[1:10,]$blog, '\\b[A-Z]+\\b')
regmatches(blog_gender_1000[1:10,]$blog, r)
str_match_all(blog_gender_1000[1:10,]$blog, '\\b[A-Z]+\\b')
str_extract(computer_531[1:20,], "[^##]*")
# gsub("[^##]*", "\\1", computer_531[1:20,])
gsub("(.?)(##.*)", "\\1", computer_531[1:20,])
computer_531[1:20,]
str_extract(computer_531[1:20,], "[^##]*")
pull(computer_531[1:20,])
str_extract(pull(computer_531[1:20,]), "[^##]*")
pull(computer_531[1:20,])
lapply(pull(computer_531[1:20,]), paste)
paste(pull(computer_531[1:20,]), collapse = "")
str_extract(paste(pull(computer_531[1:20,]), collapse = ""), "[^##]*")
?str_extract
str_extract(paste(pull(computer_531[1:20,]), collapse = ""), "[^##]*")
paste(pull(computer_531[1:20,]), collapse = "") %>% str_extract("[^##]*")
str_extract(computer_531[1:20,], "[^##]*")
computer_531[1:20,]
typeof(paste(pull(computer_531[1:20,]), collapse = ""))
paste(pull(computer_531[1:20,]), collapse = "")
computer_531_begin = paste(pull(computer_531[1:20,]), collapse = "")
str_extract(computer_531_begin , "[^##]*")
computer_531_begin
str_extract(computer_531_begin , "[^##]*", simplify = TRUE)
shopping_list <- c("apples x4", "bag of flour", "bag of sugar", "milk x2")
str_extract(shopping_list, "\\d")
computer_531 <- data.frame(computer_data)
computer_data <- readLines("computer.txt")
```{r 12, include = TRUE, warning = FALSE}
computer_data <- readLines("computer.txt")
computer_531 <- data.frame(computer_data)
head(computer_531)
computer_531[1:20,]
str_extract(computer_531[1:20,], "[^##]*")
gsub("(.?)(##.*)", "\\1", computer_531[1:20,])
?gsub
computer_531$aspect_sentiment <- gsub("(.?)(##.*)", "\\1", computer_531[,])
head(computer_531)
## keep only reviews:
computer_531$review <- sub('(.*)(##?)', '', computer_531[,]$computer_data)
computer_531
head(computer_531)
computer_531
computer_531$aspect_sentiment
computer_531$aspects <- gsub("\\[.+?\\]", "", computer_531$aspect_sentiment)
head(computer_531)
# computer_531 <- computer_531 %>%
#  mutate(aspects = gsub("\\[.+?\\]", "", computer_531$aspect_sentiment))
# only for +1 and -1
# gsub("\\[(.+?)1\\]", "", "a yes[+1][a]")
# gsub("\\[(\\+|-)1\\]", "", "a yes[+1][a]")
head(computer_531)
computer_531 <- computer_531 %>%
mutate(sentiment_score = 0)
for(i in 1:nrow(computer_531)){
score_list <- str_extract_all(computer_531[i,]$aspect_sentiment, "-?\\d+")
if(length(score_list[[1]]) != 0){
computer_531[i,]$sentiment_score <- sum(as.numeric(as.character(unlist(score_list)[[1]])))
}
}
computer_531 <- computer_531 %>%
mutate(sentiment = "neutral")
for(i in 1:nrow(computer_531)){
if (as.numeric(computer_531[i,]$sentiment_score) < 0) {
computer_531[i,]$sentiment <- "negative"
}
else if(as.numeric(computer_531[i,]$sentiment_score) > 0) {
computer_531[i,]$sentiment <- "positive"
}
}
head(computer_531)
write_csv(computer_531, "computer_531.csv")
head(computer_531)
