---
title: "Preprocessing text"
author: "Mees van Stiphout"
date: "11 July 2022"
output:
  beamer_presentation: default
  html_document:
    number_sections: no
    toc: yes
    toc_depth: 5
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '5'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
fig.align = 'center'
```

# This afternoon

# Insight: Cleaning of data and text is a vitally important skill set.

## Bag of Words

```{r fig.width=8, fig.height=5, echo = FALSE}
library(jpeg)
img <- readJPEG("Bagofwords.jpg")
#grid.raster(img)
```

## POStagging

```{r fig.width=8, fig.height=5, echo = FALSE}
img <- readJPEG("posgrammar.JPG")
#grid.raster(img)
```

## Subjects

-   Purposes of text cleaning

-   Getting rid of noise and stop words

-   Tokenizing and Ngrams

-   Stemming and lemmatizing

-   Language guesser

-   POS tagging!


## Install or call relevant libraries/packages {.smaller}

```{r load_packages, message = FALSE, warning = FALSE}
library (cld2)         # for language detection
library (corpus)       # for text analysis with support for international text
library (dplyr)         # for data manipulation
library (entity)        # for easy NER 
library (ggraph)       # for graphs
library (gridExtra)     # for working with grids to obtain nice layouts
library (hunspell)     # for high-performance stemming
library (igraph)       # for easy graphs 
library (janitor)      # for a pretty table
library (lattice)      # for easy charts
library (knitr)        # for manipulating the output of this markdown document
library (plyr)         # for data manipulation
library (NLP)          # for natural language processing tools
library (openNLP)      # for named entity recognition
library (pander)       # for nice slide output
library (qdap)         # for using parsing tools to prepare transcript data

```

```{r load_packages_continued, message = FALSE, warning = FALSE, echo = FALSE}
library (jpeg)
library (grid)
library (RColorBrewer) # for extra color schemes for visual output
library (rJava)        # for using openNLP
library (markdown)     # for formatting documents like this one
library (SnowballC)    # for Porter's stemming algorithm
library (stringr)      # for working with strings
library (textstem)     # for stemming and Lemmatizing text
library(tidyverse)     # for tidy data and pipes
library (tm)           # for text mining
library (udpipe)       # for natural language processing and annotation
library(wordcloud)     # for creating pretty word clouds
```

## Point to the right working directory:

```{r}
#setwd("C:/Noodfolder")
```

## Read CSV file:

- Articles on John Maynard Keynes taken from The Times newspaper

- read.csv ("Keynes.csv")

```{r, echo = TRUE }
Keynes <- read.csv("Keynes.csv", stringsAsFactors = FALSE)
```

## Dataframe Times articles
```{r fig.width=9, fig.height=5, echo = FALSE}
img <- readJPEG("DataframeKeynesarticles.JPG")
#grid.raster(img)
```


## EXAMPLE TEXT FIELD

-   Title: Mr. Roosevelt's Experiments
-   Publication date: February 27, 1940


```{r}
TEXT <- "HOW TO PAY FOR THE 3WAR. By JOHN\nMAYNARD KEYNES. Macmillan. Is.
\nIn three articles published i6n Tlhe Times last November Mr. J. M.
Keynes put forward what was expressly a first draft of proposals f6or
compulsory savings in  wartime. 3388 His plan  now reappears in the form
of  a pamphlet in which Mr. Keynes elaborates his earlier argument and
varies it to meet the  comment and suggestion which it has so widely
provoked.The pamphlet includes four appendices on the national income5the 
extent of our resources abroad the cost of family allowances and\nthe formula 
for the aggregate of deferred pay and direct taxes\nthe\npamphlet  
is the subject of a leading article in 344 todays issue"
```

## EXAMPLE TEXT FIELD Tokenized

```{r, echo=TRUE}
TEXTtokens <- str_split(TEXT, " ")
TEXTtokens <- unlist(TEXTtokens)
print (TEXTtokens [1:50])
```

## Wordcloud of tokens

```{r, echo = TRUE, warning=FALSE}
wordcloud(TEXTtokens, scale = c(4,.5), min.freq =1, colors = brewer.pal(1, "Dark2"))
```

## Clean text: all words in lower case

```{r, echo=TRUE}
TEXT2 <- tolower(TEXT)
cat(TEXT2)
```

## Remove numbers

```{r}
TEXT2 <- tm::removeNumbers(TEXT2)
cat(TEXT2)
```


## Remove double spaces

```{r}
TEXT2<- str_replace_all(TEXT2, "  ", " ")
cat(TEXT2)
```

## Remove punctuation

```{r}
TEXT2<-tm::removePunctuation(TEXT2)
cat(TEXT2)
```

## Is the number of tokens reduced?

```{r}
testtokens <- text_ntoken(TEXT)
print(testtokens)
testtokens2 <- text_ntoken(TEXT2)
print(testtokens2)
```

## CLEAN THE TEXTS IN KEYNES DATAFRAME {.smaller}

```{r}
#All words in lower case  
cleancontent <- tolower(Keynes$content)

## Remove numbers  

cleancontent <- tm::removeNumbers(cleancontent)

## Remove double spaces  

cleancontent<- str_replace_all(cleancontent, "  ", " ")

## Remove punctuation

cleancontent<-tm::removePunctuation(cleancontent)

## Store results in new column in the dataframe

Keynes$clean_content <- cleancontent

```

## Reduction in the number of tokens?

```{r}
testtokens <- text_ntoken(Keynes$content)
print (sum(testtokens))

testtokens2 <- text_ntoken(Keynes$clean_content)
print (sum(testtokens2))
```

# stopwords

## Store all too common words in a variable


```{r}
#Stopwordslist  
  
Meesstopwords <- c("the", "a", "an", "and", "i","in", "ïf", "he","she",
                     "him", "should", "or", "we", "no", "over", "only",
                     "could", "to", "of", "it", "is", "that", "this",
                     "was", "/'s", "with","/'t", "as", "'t", "on", "are",
                     "one", "so", "be","me", "are", "at", "s", "has",
                     "by", "/", "for", "not", "from", "have", "which",
                     "but", "his","than", "their", "were", "any", "some",
                     "can", "what", "will", "would", "been", "per",
                     "more", "they", "there", "tibble", "its", "had",
                     "mrs", "mr",   "when", "all", "our", "cent", "her",
                     "who") 

```

## Remove these words:
```{r}
Keynestext <- removeWords(Keynes$clean_content, Meesstopwords)
#And store clean text in a new column
Keynes$clean_content <- Keynestext
```

<!-- ## Clean content in dataframe -->
<!-- ![](Cleancontent.JPG) -->

## Original content

```{r, warning =FALSE, echo = FALSE}
pal <- brewer.pal(n = 15, name = "PRGn")
wordcloud(Keynes$content, scale = c(5,.7), min.freq = 80, colors = brewer.pal(1, "Dark2"))
```

## Clean content
```{r, warning =FALSE, echo = FALSE}
wordcloud(Keynes$clean_content, scale = c(3,.5), min.freq = 70, colors = brewer.pal(1, "Dark2"))
```
:::

# STEMMING AND LEMMATIZATION

## Stemming:

SnowballC library uses Porter's word stemming algorithm that collapses
words to a common root. It supports many languages:

```{r}
getStemLanguages()
```

## Examples

```{r}
wordStem("compulsary")
wordStem("articles")
wordStem("published")
wordStem("November")

```

## Stem example text

```{r echo = FALSE, warning=FALSE}
test <- stem_words(TEXT2, language = "porter")
print(test)

```

how to pay for the war by john maynard keynes macmillan Is in three
articles published in the times last november mr j m keynes put forward
what was expressly a first draft of proposals for compulsory savings in
wartime

how to pay for the war by john maynard keyn macmillan is in three articl
publish in the time last novemb mr j m keyn put forward what was
expressli a first draft of propos for compulsori save in wartim



# Lemmatization:

## Make lemma dictionary 

```{r}
make_lemma_dictionary(TEXT2)
```

## Lemmatize (cleaned) example text
```{r, warning=FALSE, message = FALSE}
Lemmatext <- lemmatize_strings(TEXT2, dictionary = lexicon::hash_lemmas)
```
<!-- ![](Lemmatext.JPG) -->


## Keynes Top 20 of (meaningful) words

```{r, echo = FALSE, oud.with = "100%"}
Keynestop20 <- freq_terms(Keynes$clean_content, top = 20, at.least = 3)

My_Theme = theme(
  axis.title.x = element_text(size = 12),
  axis.text.x = element_text(size = 12),
  axis.title.y = element_text(size =12),
  axis.text.y = element_text(size = rel (1.4)))
  
ggplot(data=Keynestop20, mapping = aes(x= reorder(WORD, FREQ), FREQ)) +
    geom_bar(stat="identity", fill = "red") + My_Theme + coord_flip() 
```


# Language detection

## Several language detectors available

- Package cld = Compact Language Detector (Google)
- several versions (2, 3)

## Use CLD2 to detect language in Good Reads reviews
```{r}
GRreviewssc <- read.csv("CLDdemo2.csv", sep = ";")
GRreviewssc$langdtcd <- detect_language(GRreviewssc$Text, 
plain_text = TRUE, lang_code = TRUE)
```

```{r, echo = FALSE, warning = FALSE}
GRreviewssc <- select(GRreviewssc, ID, Text, Language, langdtcd)
```

<!-- ##  Result -->
<!-- ![](Langguess.JPG) -->

# POSTAGGING

## Example text

```{r, echo = FALSE, warning = FALSE}
cat(TEXT2)
```

<!-- ## ![](Taggedtext.JPG) -->


## Tagging of content field of Keynes dataframe (package: Udpipe)


```{r, message=FALSE}
ud_model <- udpipe_download_model(language = "english")
```

## Part of speech tagging of the clean content field of the Keynes dataframe.

```{r}
ud_english <- udpipe_load_model(ud_model$file_model)
cleanfieldPOS <- udpipe_annotate(ud_english, x = Keynes$clean_content, doc_id = Keynes$date.pub)
cleanfieldPOS <- as.data.frame(cleanfieldPOS)
```

<!-- ## This is what the resulting dataframe looks like: -->
<!-- ![](POScontfielddataframe.JPG) -->

## Results barchart

```{r}
statsclean <- txt_freq(cleanfieldPOS$upos)
statsclean$key <- factor(statsclean$key, levels = rev(statsclean$key))
barchart(key ~ freq, data = statsclean, col = "cadetblue", 
main = "UPOS (Universal Parts of Speech)\n frequency of occurrence", 
 xlab = "Frequency")
```


## Which nouns are most important?

```{r}
nounstat <- subset(cleanfieldPOS, upos %in% c("NOUN")) 
nounstat <- txt_freq(nounstat$token)
```

```{r, echo = FALSE}
nounstat$key <- factor(nounstat$key, levels = rev(nounstat$key))
barchart(key ~ freq, data = head(nounstat, 30), col = "cadetblue", 
         main = "Most occurring nouns", xlab = "Freq")
```


## And adjectives:

```{r, echo = FALSE}
adjstat <- subset(cleanfieldPOS, upos %in% c("ADJ")) 
adjstat <- txt_freq(adjstat$token)

adjstat$key <- factor(adjstat$key, levels = rev(adjstat$key))
barchart(key ~ freq, data = head(adjstat, 30), col = "cadetblue", 
         main = "Most frequent adjectives", xlab = "Freq")
```

# Keyword combinations in text

## Identifying keywords with RAKE
```{r}
keyrake <- keywords_rake(x = cleanfieldPOS, term = "lemma", group = "doc_id", 
                       relevant = cleanfieldPOS$upos %in% c("NOUN", "ADJ"))
```


```{r, echo = FALSE}
keyrake$key <- factor(keyrake$keyword, levels = rev(keyrake$keyword))
barchart(key ~ rake, data = head(subset(keyrake, freq > 3), 20), col = "cadetblue", 
         main = "Keywords identified by RAKE", 
         xlab = "Rake")
```

# NER = Named Entity Recognition

## Tags for NER

Correct NER tagging requires a model for the language used. Wirtschafts
universität in Vienna offers models on this website:
<https://datacube.wu.ac.at/>.

## The Wikipedia text on the first election debate with Biden and Trump

```{r}
President <- readLines('Debat.txt')
President <- as.String(President)
```

## Wrapper around library

Open NLP needs the file to be tokenized, either in words or in sentences
and the result will be stored as a list.

```{r}
sent_ann <- Maxent_Sent_Token_Annotator()
```

## Annotated text to replace the original

```{r}
President_sent <- NLP::annotate(President, list(sent_ann))
President_nmbrd <- President[President_sent]
President_nmbrd
```

<!-- ## Persons mentioned in the text -->

<!-- ```{r, echo = FALSE, warning = FALSE} -->
<!-- personen <- person_entity(President_nmbrd) -->
<!-- plot(personen) -->

<!-- ``` -->

<!-- ## And how about organizations? -->

<!-- ```{r,echo = FALSE, warning = FALSE} -->
<!-- organizations <- organization_entity(President_nmbrd) -->
<!-- plot(organizations) -->
<!-- ``` -->

# Questions?

# Practical
