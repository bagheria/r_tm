---
title: "Text Clustering and Topic Modeling"
mainfont: Arial
fontsize: 12pt
urlcolor: blue
output: 
  html_document:
    code_folding: hide
    toc: true
    toc_depth: 1
    toc_float: true
    df_print: paged
    theme: paper
    pandoc_args: --output=Practical_5.html
---

# Introduction

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE)
```

Welcome to the fifth practical of the course "Introduction to Text Mining with R". In this practical, we will apply a clustering method and a topic modeling approach on news articles and cluster them into different categories. 

In this practical we are going to use the following packages:

```{r load_packages, message = FALSE, warning = FALSE}
library(tm)
library(tidytext)
library(dplyr)

```

---

# 

---

---

## Read data

---

1. **The dataset that we are going to use is the BBC dataset from practical 3. The BBC dataset consists of 2225 documents and 5 categories: business, entertainment, politics, sport, and tech. For text clustering and topic modeling, we will ignore the labels but we will use them while evaluating models. Load the Rda file of the news dataset.**

---

```{r 1, include = TRUE}
load("data/news_dataset.rda")
head(df_final)

```

---

2. **First we must preprocess the corpus. Create a document-term matrix from the `news` column of the dataframe. Complete the following preprocessing steps:**
- convert to lower
- remove stop words
- remove numbers
- remove punctuation
- use tfidf weighting
- convert dtm to a dataframe

---


```{r 2, include = TRUE}
docs <- Corpus(VectorSource(df_final$Content))

dtm <- DocumentTermMatrix(docs,
           control = list(tolower = TRUE,
                          removeNumbers = TRUE,
                          removePunctuation = TRUE,
                          stopwords = TRUE
                         ))

#dtm <- as.data.frame(as.matrix(dtm))
dtm <- as.matrix(dtm)

```

---

3. **Now we can run a k-means clustering method, starting out with three centers. What does the output look like?**

---

```{r 3, include = TRUE}
text_kmeans_clust3 <- kmeans(dtm, centers = 3)
summary(text_kmeans_clust3)

# Cosine distance matrix (useful for specific clustering algorithms) 
library(proxy)
dist_matrix <- dist(dtm, method = "cosine")

clustering.hierarchical <- hclust(dist_matrix, method = "ward.D2") 
plot(clustering.hierarchical, cex=0.9, hang=-1)
rect.hclust(clustering.hierarchical, k=5)

clustering.dbscan <- dbscan::hdbscan(dist_matrix, minPts = 10)

library(broom)
tidy(text_kmeans_clust3)

```

The original format of the output isn’t as practical to deal with in many circumstances, so we can load the broom package (part of tidymodels) and use verbs like tidy(). This will give us the centers of the clusters we found:

If we augment() the clustering results with our original data, we can plot any of the dimensions of our space, such as total employed vs. proportion who are Black. We can see here that there are really separable clusters but instead a smooth, continuous distribution from low to high along both dimensions. Switch out another dimension like asian to see that projection of the space.

```{r}
text_kmeans_clust3 %>%
  ggplot(aes(also, advert, color = .cluster)) +
  geom_point()

```


---

## Clustering methods
## Choosing k

---

4. ** **

We used k = 3 but how do we know that’s right? There are lots of complicated or “more art than science” ways of choosing k. One way is to look at the total within-cluster sum of squares and see if it stops dropping off so quickly at some value for k. We can get that from another function from the broom package, glance(); let’s try lots of values for k and see what happens to the total sum of squares.



---

```{r 4, include = TRUE, warning = FALSE}
kclusts <-
  tibble(k = 1:9) %>%
  mutate(
    kclust = map(k, ~ kmeans(dtm, .x)),
    glanced = map(kclust, glance),
  )

kclusts %>%
  unnest(cols = c(glanced)) %>%
  ggplot(aes(k, tot.withinss)) +
  geom_line(alpha = 0.5, size = 1.2, color = "midnightblue") +
  geom_point(size = 2, color = "midnightblue")


```

---

5. **Fit k-means again with the optimal k.**

---

```{r 5, include = TRUE, warning = FALSE}
final_clust <- kmeans(select(employment_demo, -occupation), centers = 5)

```

---

---

##

---

6. **Visualise the results**

To visualize this final result, let’s use plotly and add the occupation name to the hover so we can mouse around and see which occupations are more similar.
---

```{r 6, include = TRUE}
library(plotly)

p <- augment(text_kmeans_clust3, dtm) %>%
  ggplot(aes(total, women, color = .cluster, name = occupation)) +
  geom_point()

ggplotly(p, height = 500)

dtm

```

---

7. **.**

---

```{r 7, include = TRUE, warning = FALSE}


```

---

8. **.**

---

```{r 8, include = TRUE, warning = FALSE}


```


---

9. ** .**

---

```{r 9, include = TRUE, warning = FALSE}


```

---

10. **.**

---

```{r 10, include = TRUE, warning = FALSE}

```

---

#

---

11. **Load the final `computer_531` dataframe from the first practical and  to read data from the 'computer.txt' file.**

---

```{r 11, include = TRUE, warning = FALSE}

```

---

12. **Apply the `sentiment_score` function on the reviews in the dataframe with both the bing and afinn dictionaries.**

---

```{r 12, include = TRUE, warning = FALSE}

```


---

13. **Create a confusion matrix. Accuracy, precision, recall and F1 measures.**

---

```{r 13, include = TRUE}

```

---

14. **From the `neuralnet` package, we want to apply a multi-layer perceptron to this dataset and train a model for prediction of the sentiments.**

---

```{r 14, include = TRUE}


```

---

15. **Compare nn1, bing and affine ...**

---


```{r 15, include = TRUE}

```

---

16. **Visualise the results**

---

```{r 16, include = TRUE}

```


---



---

# Summary

---



---

End of Practical

---
